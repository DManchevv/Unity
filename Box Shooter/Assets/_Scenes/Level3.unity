rt1_out], v[voffset_out], v[vtmp] // +hw_off

    v_mul_u32_u24 v[vtmp], 0 + input_w_stride * part2_chunks, v[vtmp2]
    _v_add_nc_u32 v[voffset_part2_in],  v[voffset_in], v[vtmp] // +hw_off
    v_mul_u32_u24 v[vtmp], 0 + output_w_stride * part2_chunks, v[vtmp2]
    _v_add_nc_u32 v[voffset_part2_out], v[voffset_out], v[vtmp] // +hw_off

    v_mul_u32_u24 v[vtmp], 0 + input_w_stride * elements_per_lane, v[vtmp2]
    _v_add_nc_u32 v[voffset_in], v[voffset_in], v[vtmp] // +hw_off
    v_mul_u32_u24 v[vtmp], 0 + output_w_stride * elements_per_lane, v[vtmp2]
    _v_add_nc_u32 v[voffset_out], v[voffset_out], v[vtmp] // +hw_off

    s_mul_i32 s[stmp], 0 + 4 * read_size * wave_size, s[wave_id]

    // calculate buffer scalar offsets
    s_mul_i32 s[c_base], 0 + c_per_gpr * c_mult, s[gid_y]
    s_mul_i32 s[k_base], 0 + k_per_gpr * k_mult, s[gid_z]
    s_mul_i32 s[n_base], 0 + n_per_gpr, s[wave_id]

    s_mul_i32 s[soffset_in], 0 + input_n_stride, s[n_base]
    s_mul_i32 s[stmp], 0 + input_c_stride, s[c_base]
    s_add_u32 s[soffset_in], s[soffset_in], s[stmp]

    s_mul_i32 s[soffset_out], 0 + output_n_stride, s[n_base]
    s_mul_i32 s[stmp], 0 + output_k_stride, s[k_base]
    s_add_u32 s[soffset_out], s[soffset_out], s[stmp]

    s_mul_i32 s[soffset_wei], 0 + filter_c_stride, s[c_base]
    s_mul_i32 s[stmp], 0 + filter_k_stride, s[k_base]
    s_add_u32 s[soffset_wei], s[soffset_wei], s[stmp]


    // mask unused lanes
    _v_add_nc_u32 v[c_id], s[c_base], v[c_id]
    _v_add_nc_u32 v[k_id], s[k_base], v[k_id]
    _v_add_nc_u32 v[n_id], s[n_base], v[n_id]
    v_cmp_gt_u32 vcc, 0 + input_channels, v[c_id]
    v_cndmask_b32_e32 v[voffset_in], -1, v[voffset_in], vcc
    v_cmp_gt_u32 vcc, 0 + output_channels, v[k_id]
    v_cndmask_b32_e32 v[voffset_out], -1, v[voffset_out], vcc

    v_mov_b32 v[vtmp], 0x7FFFFFFF
    v_cmp_gt_u32 vcc, 0 + active_lanes_in_full_chunks, v[vtmp2]
    v_cndmask_b32_e32 v[voffset_in],  v[vtmp], v[voffset_in], vcc
    v_cndmask_b32_e32 v[voffset_out], v[vtmp], v[voffset_out], vcc

    .if(adv_perf_param_comb)
        v_mov_b32 v[voffset_part2_in],  v[voffset_part1_in]
        v_mov_b32 v[voffset_part2_out],  v[voffset_part1_out]
    .else
        v_cmp_gt_u32 vcc, 0 + active_lanes_in_part2_chunks, v[vtmp2]
        v_cndmask_b32_e32 v[voffset_part2_in],  v[vtmp], v[voffset_part2_in], vcc
        v_cndmask_b32_e32 v[voffset_part2_out], v[vtmp], v[voffset_part2_out], vcc
    .endif

    v_cmp_gt_u32 vcc, 0 + active_lanes_in_part1_chunks, v[vtmp2]
    v_cndmask_b32_e32 v[voffset_part1_in],  v[vtmp], v[voffset_part1_in], vcc
    v_cndmask_b32_e32 v[voffset_part1_out], v[vtmp], v[voffset_part1_out], vcc

    v_cmp_eq_u32 vcc, 0 + active_lanes_in_part1_chunks, v[vtmp2]
    s_nop 4
    s_mov_b64 s[bound_lanes_exec:bound_lanes_exec+1], vcc

    .GPR_INVALIDATE c_id
    .GPR_INVALIDATE k_id
    .GPR_INVALIDATE vtmp
    .GPR_INVALIDATE vtmp2

    // fill format and size fields of buffer descriptors
    s_mov_b32 s[desc_in+2], input_buffer_size
    s_mov_b32 s[desc_in+3], 0x00027000
    s_mov_b32 s[desc_wei+2], filter_buffer_size
    s_mov_b32 s[desc_wei+3], 0x00027000
    s_mov_b32 s[desc_out+2], output_buffer_size
    s_mov_b32 s[desc_out+3], 0x00027000

    i = 0
    .rept accums_cnt
        v_mov_b32 v[accums+i], 0
        i = i + 1
    .endr

    s_waitcnt 0

    // calculate buffer offsets
    s_add_u32 s[desc_wei], s[desc_wei], s[soffset_wei]
    s_addc_u32 s[1+desc_wei], 0, s[1+desc_wei]
    s_sub_u32 s[2+desc_wei], s[2+desc_wei], s[soffset_wei]
    s_max_i32 s[2+desc_wei], 0, s[2+desc_wei]

    // compute permute_addr
    .if c_quads == 4
        _v_add_nc_u32 v[permute_addr], 0 + wave_size / k_ds_rotates, v[tid]
        v_lshlrev_b32 v[permute_addr], 2, v[permute_addr]
    .endif

    s_mov_b32 s[loop_n_cnt], s[n_base]

        .macro increase_ioffset_or_soffset i_offset, sgpr, sgpr_offset, rize_vall
        _buff = \i_offset + \rize_vall
        .if(_buff >= (1 << 12))
            s_add_u32 s[\sgpr], s[\sgpr], 0 + _buff
            \i_offset = 0
            \sgpr_offset = \sgpr_offset + _buff
        .else
            \i_offset = \i_offset + \rize_vall
        .endif
    .endm

    .macro m_load inout, total_adj, dwords1, voff1, w_cnt, ld_id, dwords2=0, voff2=0, shorts1=0, shorts2=0, ex_load=0
        .if lines_\inout == lines_in
            sequential_output_channels = sequential_c_channels
            ck_stride = input_c_stride
            mult = c_mult
            dst = lines_in + \ld_id * inbuf_prefetch_vgpr_offset
            desc = desc_in
            soff = soffset_in
            adj_size = c_per_gpr * input_c_stride
        .else
            mult = k_mult
            sequential_output_channels = sequential_k_channels
            ck_stride = output_k_stride
            dst = lines_out + \ld_id * outbuf_prefetch_vgpr_offset
            desc = desc_out
            soff = soffset_out
            adj_size = k_per_gpr * output_k_stride
        .endif

         mult = mult / sequential_output_channels
        adj_size = adj_size * sequential_output_channels

        _mult_it = 0
        .rept mult
            _sequential_output_channels_it = 0
            _sequential_ck_offset = 0
            _seq_ck_offset_in_soffset = 0
            .rept sequential_output_channels
                .if(_sequential_output_channels_it != 0)
                    increase_ioffset_or_soffset _sequential_ck_offset, soff, _seq_ck_offset_in_soffset, ck_stride
                .endif
                m_buffer_load_dwordx \dwords1, dst,            \voff1, desc, soff, _sequential_ck_offset
                .if(\dwords1>0)
                    \w_cnt = \w_cnt + 1
                .endif

                .if(\shorts1 != 0)
                    short_offset = \dwords1 * dword_size
                    m_buffer_load_ushort 1, \dwords1 + dst, \voff1, desc, soff, (_sequential_ck_offset + short_offset)
                    \w_cnt = \w_cnt + 1
                .else
                    .if(\ex_load)

                        s_mov_b64 exec, s[bound_lanes_exec:bound_lanes_exec+1]
                        bound_dwords_cnt = bound_elements_cnt / elements_in_dword
                        bound_shorts_cnt = bound_elements_cnt % elements_in_dword

                        m_buffer_load_dwordx bound_dwords_cnt, dst, \voff2, desc, soff, _sequential_ck_offset
                        short_offset = bound_dwords_cnt * dword_size
                        m_buffer_load_ushort bound_shorts_cnt, dst + bound_dwords_cnt, \voff2, desc, soff, (_sequential_ck_offset + short_offset)

                        .if(bound_dwords_cnt )
                            \w_cnt = \w_cnt + 1
                        .endif
                        .if(bound_shorts_cnt)
                            \w_cnt = \w_cnt + 1
                        .endif

                        s_mov_b64 exec, -1
                    .else
                        m_buffer_load_dwordx \dwords2, dst + \dwords1, \voff2, desc, soff, (_sequential_ck_offset + part2_offset)
                        .if(\dwords2 > 0)
                            \w_cnt = \w_cnt + 1
                        .endif

                        .if(\shorts2 != 0)
                            short_offset = part2_offset + part2_dwords * dword_size
                            m_buffer_load_ushort 1, dst + \dwords1 + \dwords2, \voff2, desc, soff, (_sequential_ck_offset + short_offset)
                            \w_cnt = \w_cnt + 1
                        .endif
                    .endif
                .endif

                dst = dst + read_size //\dwords1
                _sequential_output_channels_it = _sequential_output_channels_it + 1
            .endr
            .if(_mult_it != (mult - 1))

                s_add_u32 s[soff], s[soff], 0 + adj_size - _seq_ck_offset_in_soffset
                \total_adj = \total_adj + adj_size
                _mult_it = _mult_it + 1
            .else
                \total_adj = \total_adj + _seq_ck_offset_in_soffset
            .endif
        .endr
    .endm

.if(full_reads > 0 && full_reads < data_prefetch + 1)
    data_prefetch = full_reads - 1
.endif

LD_PARTIAL_CHUNKS = 1
LD_FULL_CHUNKS = 0
LD_PART_A_ID = 0
LD_PART_B_ID = data_prefetch
last_free_ld_part = LD_PART_B_ID


.macro ld_buffers_inc_pointers_rept_waitcnt chunk_type, ld_part_id, rept_wait_cnt=-1
     wait_cnt = 0

    .if(\chunk_type == LD_PARTIAL_CHUNKS)
        m_load in,  c_off, part1_dwords, voffset_part1_in, wait_cnt, \ld_part_id, part2_dwords, voffset_part2_in, part1_shorts, part2_shorts, adv_perf_param_comb
        m_load out, k_off, part1_dwords, voffset_part1_out, wait_cnt, \ld_part_id, part2_dwords, voffset_part2_out, part1_shorts, part2_shorts, adv_perf_param_comb
    .else
        c_off = 0
        k_off = 0
        m_load in,  c_off, (elements_per_lane / elements_in_dword), voffset_in, wait_cnt, \ld_part_id
        m_load out, k_off, (elements_per_lane / elements_in_dword), voffset_out, wait_cnt, \ld_part_id
        s_add_u32 s[soffset_in],  s[soffset_in],  0 + active_lanes_in_full_chunks * (elements_per_lane * input_w_stride) - c_off
        s_add_u32 s[soffset_out], s[soffset_out], 0 + active_lanes_in_full_chunks * (elements_per_lane * output_w_stride) - k_off
    .endif
    .if(\rept_wait_cnt != -1)
        \rept_wait_cnt = wait_cnt
    .endif
.endm

.macro data_prefetch_init_q q_wait_cnt, singl_wait_cnt
    q_id = LD_PART_A_ID
    .rept data_prefetch
        ld_buffers_inc_pointers_rept_waitcnt LD_FULL_CHUNKS, q_id, \singl_wait_cnt
        \q_wait_cnt = \q_wait_cnt + \singl_wait_cnt
        q_id = (q_id + 1)
    .endr
.endm

.macro data_ld_prefetch_active_loop q_wait_cnt, loop_cnt=data_prefetch+1
    q_id_conv = LD_PART_A_ID
    q_id_data_ld = LD_PART_B_ID
    .rept \loop_cnt
        ld_buffers_inc_pointers_rept_waitcnt LD_FULL_CHUNKS, q_id_data_ld

        s_wait \q_wait_cnt

        m_conv_accums elements_per_lane, q_id_conv

        q_id_conv = ((q_id_conv + 1) % (data_prefetch + 1))
        q_id_data_ld = ((q_id_data_ld + 1) % (data_prefetch + 1))
    .endr
.endm

.macro data_prefetch_conv_finalizing q_wait_cnt, singl_wait_cnt, q_id_conv_off=0
    q_id_conv = ((LD_PART_A_ID + \q_id_conv_off) % (data_prefetch + 1))

    .rept data_prefetch
        \q_wait_cnt = (\q_wait_cnt - \singl_wait_cnt)
        s_wait \q_wait_cnt
        m_conv_accums elements_per_lane, q_id_conv
        q_id_conv = ((q_id_conv + 1) % (data_prefetch + 1))
    .endr
.endm

S_GETPC_B64 s[loop_begin_ptr:loop_begin_ptr+1]
loop_n_begin: // loop over batch (n)
    s_mov_b32 s[loop_hw_cnt], 0

    c_off = 0
    k_off = 0
    q_wait_vec_load_full = 0
    single_wait_vec_load_full = 0
    .if full_reads
        loop_resi = 0

        data_prefetch_init_q q_wait_vec_load_full, single_wait_vec_load_full

        .if(full_reads >= 2 * data_prefetch + 1)
            loop_hw_begin:
                data_ld_prefetch_active_loop q_wait_vec_load_full

            loop_hw_end:
                s_addk_i32 s[loop_hw_cnt], 1 + data_prefetch
                s_cmpk_gt_u32 s[loop_hw_cnt], 0 + full_reads - (2 * data_prefetch + 1)
                s_cbranch_scc0 loop_hw_begin
        .endif
        .if( (full_reads - data_prefetch) % (data_prefetch + 1) != 0)
            loop_resi = ((full_reads - data_prefetch) % (data_prefetch + 1))

            data_ld_prefetch_active_loop q_wait_vec_load_full, loop_resi

            last_free_ld_part = ((LD_PART_B_ID + loop_resi) % (data_prefetch + 1))
        .endif
    .endif

    c_off = full_chunks * input_w_stride * active_lanes_in_full_chunks
    k_off = full_chunks * output_w_stride * active_lanes_in_full_chunks
    .if partial_chunks
        wait_vec_load_part = 0
        part1_dwords = part1_chunks  / elements_in_dword
        part2_dwords = part2_chunks / elements_in_dword
        part1_shorts = part1_chunks % elements_in_dword
        part2_shorts = part2_chunks % elements_in_dword
        ld_buffers_inc_pointers_rept_waitcnt LD_PARTIAL_CHUNKS, last_free_ld_part, wait_vec_load_part
        q_wait_vec_load_full = q_wait_vec_load_full + wait_vec_load_part
    .endif

    .if(full_reads != 0 && data_prefetch != 0)
        data_prefetch_conv_finalizing q_wait_vec_load_full, single_wait_vec_load_full, loop_resi
    .endif

    .if(partial_chunks)
        s_wait 0
        m_conv_accums partial_chunks, last_free_ld_part
     .endif

    s_add_u32 s[soffset_in],  s[soffset_in],  0 + input_n_stride * n_per_gpr * n_part_cnt - c_off
    s_add_u32 s[soffset_out], s[soffset_out], 0 + output_n_stride * n_per_gpr * n_part_cnt- k_off
loop_n_end:
    _v_add_nc_u32 v[n_id], 0 + (n_per_gpr * n_part_cnt) , v[n_id]

    s_addk_i32 s[loop_n_cnt], 0 + n_per_gpr * n_part_cnt
    s_cmpk_ge_u32 s[loop_n_cnt], 0 + batch_size
    //s_cbranch_scc0 loop_n_begin
    s_cbranch_scc1 loop_exit

    s_setpc_b64 s[loop_begin_ptr:loop_begin_ptr+1]
loop_exit:

    // reduction inside chunk
    m_acc_reduction 0, chunk_size_log2

    // reduction across n and hw pieces
    .GPR_REUSE voffset_out, vtmp
    .if n_per_gpr * hw_per_gpr > 1
        .if chunk_size >= 4
            v_lshlrev_b32 v[permute_addr], 2 + chunk_size_log2, v[tid]
            m_bpermute accums, accums_cnt, permute_addr
            // acc layout [n/hw][c]:
            // c0n0 c1n0 c2n0 ... c0n1 c1n1 c2n1 ...
            s_waitcnt 0 // todo: later
            m_acc_reduction c_per_gpr_log2, n_per_gpr_log2 + hw_per_gpr_log2
        .else
            v_lshrrev_b32 v[vtmp], 0 + n_per_gpr_log2 + hw_per_gpr_log2, v[tid]
            v_lshlrev_b32 v[permute_addr], 0 + c_per_gpr_log2, v[tid]
            v_and_b32 v[permute_addr], 0 + wave_size/chunk_size - 1, v[permute_addr]
            v_bfi_b32 v[permute_addr], 0 + c_per_gpr - 1, v[vtmp], v[permute_addr]
            v_lshlrev_b32 v[permute_addr], 2 + chunk_size_log2, v[permute_addr]
            m_bpermute accums, accums_cnt, permute_addr
            // acc layout [c][n/hw]:
            // c0n0 c0n1 c0n2 ... c1n0 c1n1 c1n2 ...
            s_waitcnt 0 // todo: more later

            m_acc_reduction 0, n_per_gpr_log2 + hw_per_gpr_log2

            v_lshlrev_b32 v[permute_addr], 2 + n_per_gpr_log2 + hw_per_gpr_log2, v[tid]
            m_bpermute accums, accums_cnt, permute_addr
            s_waitcnt 0 // todo: finally more later
        .endif
    .endif

    s_waitcnt 0

    .GPR_REUSE lines_in, lines_in_buffer
    .GPR_REUSE lines_out, lines_in_buffer2
    //used as one

    //use LDS to merge waves
    .macro acc_nvgpr_from_lds read_buffer, buffer_start, data_cnt

        acum_idx = \buffer_start / (n_part_cnt - 1)
        wave_idx = \buffer_start - acum_idx * (n_part_cnt - 1)
        read_it = 0

        .rept \data_cnt
            .if (wave_idx >= (n_part_cnt - 1))
                wave_idx = 0
                acum_idx = acum_idx + 1
            .endif
            .if (acum_idx >= accums_cnt)
                acum_idx = 0
            .endif

            v_add_f32 v[accums + acum_idx], v[accums + acum_idx], v[\read_buffer + read_it]

            read_it = read_it + 1
            wave_idx = wave_idx + 1
        .endr
    .endm

    .if (n_part_cnt > 1)
        lds_read_size = 1
        v_mul_u32_u24 v[voffset_ldsw], 0 + lds_element_stride * lds_read_size, v[tid]

        s_cmpk_eq_u32 s[wave_id], 0
        s_cbranch_scc1 lds_read_begin

        s_sub_u32 s[stmp], s[wave_id], 1

        s_mul_i32 s[stmp], 0 + lds_element_stride * lds_read_size * wave_size, s[stmp]
        _v_add_nc_u32 v[voffset_ldsw], s[stmp], v[voffset_ldsw]

        lds_wr_id = 0
        .rept accums_cnt
            lds_acc_off = wave_size * (n_part_cnt - 1 )* lds_element_stride * lds_wr_id
            ds_write_b32 v[voffset_ldsw], v[accums + lds_wr_id], offset:0+lds_acc_off + accums_lds
            lds_wr_id = lds_wr_id + 1
        .endr
        s_wait , 0

        s_endpgm

        lds_read_begin:
        s_barrier

        lines_io_size = read_size * (c_mult + k_mult)
        lines_in_id = 0
        first_element = 0
        lds_acc_off = 0

        .rept accums_cnt * (n_part_cnt - 1)
            .if(lines_in_id >= lines_io_size)
                s_wait , 0
                acc_nvgpr_from_lds lines_in_buffer, first_element, lines_in_id
                first_element = first_element + lines_in_id
                lines_in_id = 0
            .endif
            ds_read_b32 v[lines_in_buffer + lines_in_id], v[voffset_ldsw], offset:0+lds_acc_off + accums_lds
            lds_acc_off = lds_acc_off + wave_size * lds_element_stride * lds_read_size
            lines_in_id = lines_in_id + 1
        .endr
        s_wait , 0
        acc_nvgpr_from_lds lines_in_buffer, first_element, lines_in_id
    .endif

    .GPR_REUSE lines_in_buffer, lines_in
    .GPR_REUSE lines_in_buffer2, lines_out

    // STORE
    // prepare output addresses
    .GPR_REUSE voffset_in, voffset_wei
    .GPR_REUSE lines_in, c_off
    .GPR_REUSE lines_out, k_off
    .GPR_REUSE voffset_part1_in, c_gid
    .GPR_REUSE voffset_part2_in, k_gid
    .GPR_REUSE voffset_part1_out, c_off_masked
    .GPR_REUSE voffset_part2_out, k_off_masked
    .GPR_REUSE n_id, invalid_addr
    v_mov_b32 v[invalid_addr], 0x7FFFFFFF

    v_mul_u32_u24 v[c_gid], 0 + sequential_c_channels, v[tid]
    _v_add_nc_u32 v[c_gid], s[c_base], v[c_gid]
    v_mul_u32_u24 v[c_off], 0 + filter_c_stride * sequential_c_channels, v[tid]
    v_cmp_gt_i32 vcc, 0 + c_per_gpr, v[tid]
    v_cndmask_b32_e32 v[c_off], v[invalid_addr], v[c_off], vcc

    .macro _v_add_nc_u32_ror dst, src0, src1, ror
        .long 0x320000FA + ((\src1) << 9) + ((\dst) << 17)
        .long 0xFF012100 + \src0 + ((\ror - 1) << 8)
    .endm

    v_bfe_u32 v[k_off], v[tid], 0 + c_per_gpr_log2 - k_per_gpr_log2, 0 + k_per_gpr_log2
    _v_add_nc_u32 v[k_gid], s[k_base], v[k_off]
    v_mul_u32_u24 v[k_off], 0 + filter_k_stride * sequential_k_channels, v[k_off]

    _v_add_nc_u32 v[permute_addr], 0 + wave_size / k_ds_rotates, v[tid]
    v_lshlrev_b32 v[permute_addr], 2, v[permute_addr]

    // store accums
    k_ds = 0
    rotates_inflight = 0
    .rept k_ds_rotates

        .if k_ds > 0
            rotates_inflight = rotates_inflight - 2
            s_wait , rotates_inflight
        .endif

        kx = 0

        k_mult_packed_cnt = (k_mult + sequential_k_channels - 1) / sequential_k_channels
        .rept k_mult_packed_cnt
            v_cmp_gt_i32 vcc, 0 + output_channels - kx * k_per_gpr, v[k_gid]
            v_cndmask_b32_e32 v[k_off_masked], v[invalid_addr], v[k_off], vcc
            cx = 0

            c_mult_packed_cnt = (c_mult + sequential_c_channels - 1) / sequential_c_channels
            .rept c_mult_packed_cnt
                v_cmp_gt_i32 vcc, 0 + input_channels - cx * c_per_gpr, v[c_gid]
                v_cndmask_b32_e32 v[c_off_masked], v[invalid_addr], v[c_off], vcc
                k_dpp = 0
                .rept k_dpp_rotates
                    b = (k_dpp * c_per_gpr / k_per_gpr) % 16 // lanes to ror
                    .if b == 0
                        _v_add_nc_u32 v[voffset_wei], v[k_off_masked], v[c_off_masked]
                    .else
                        .if (.option.machine_version_major == 8) // workaround for asm
                            _v_add_nc_u32_ror voffset_wei, k_off_masked, c_off_masked, b
                        .else
                            _v_add_nc_u32 v[voffset_wei], v[k_off_masked], v[c_off_masked] row_ror:b
                        .endif
                    .endif

                    acc = accums + k_per_gpr * (cx * k_mult + kx) + k_ds * k_dpp_rotates
                    .if( (buf_type == TYPE_FP16 || buf_type == TYPE_BFP16) && acc_type == TYPE_FP32)
                        acc2_cx = cx + sequential_c_channels - 1
                        acc2_kx = kx + sequential_k_channels - 1
                        acc2 = accums + k_per_gpr * ( (acc2_cx * k_mult) + acc2_kx) + k_ds * k_dpp_rotates
                        .if(buf_type == TYPE_FP16)
                            .if (!short_store)
                                v_cvt_pkrtz_f16_f32 v[acc+k_dpp], v[acc + k_dpp], v[acc2 + k_dpp]
                            .else
                                v_cvt_f16_f32 v[acc+k_dpp], v[acc + k_dpp]
                            .endif
                        .else 
                            v_lshrrev_b32 v[acc + k_dpp], 16, v[acc + k_dpp]
                            .if (!short_store)
                                v_and_b32 v[acc2 + k_dpp], 0xFFFF0000, v[acc2 + k_dpp]
                                v_or_b32 v[acc + k_dpp], v[acc + k_dpp], v[acc2 + k_dpp]
                            .endif
                        .endif
                    .endif
                    s_mov_b32 s[stmp], 0 + cx * c_per_gpr * filter_c_stride + kx * k_per_gpr * filter_k_stride

                    .if(short_store)
                        buffer_store_short v[acc+k_dpp], v[voffset_wei], s[desc_wei:desc_wei+3], s[stmp] offen
                    .else
                        buffer_store_dword v[acc+k_dpp], v[voffset_wei], s[desc_wei:desc_wei+3], s[stmp] offen
                    .endif

                    k_dpp = k_dpp + 1
                .endr
                cx = cx + sequential_c_channels
            .endr
            kx = kx + sequential_k_channels
        .endr
        k_ds = k_ds + 1

        .if k_ds < k_ds_rotates
            static_assert (c_quads == 2 || c_quads == 4)
            .if c_quads == 2
                ds_swizzle_b32 v[k_off], v[k_off] offset:0xc200
                ds_swizzle_b32 v[k_gid], v[k_gid] offset:0xc200
            .elseif c_quads == 4
                ds_bpermute_b32 v[k_off], v[permute_addr], v[k_off]
                ds_bpermute_b32 v[k_gid], v[permute_addr], v[k_gid]
            .endif
            rotates_inflight = rotates_inflight + 1
        .endif
    .endr

s_endpgm


.Lfunc_end0:
    .size gcnAsmConv1x1WrW, .Lfunc_end0 - gcnAsmConv1x1WrW

.ifndef ROCM_METADATA_VERSION
.error "ROCM_METADATA_VERSION must be defined"
.end
.endif

.ifdef n_per_group
.error "n_per_group must NOT be defined"
.end
.endif
.set n_per_group, n_part_cnt

.macro METADATA wg_x, lds_size
  .if ROCM_METADATA_VERSION == 4
    .amd_amdgpu_hsa_metadata
    { Version: [ 1, 0 ],
        Kernels:
        - { Name: gcnAsmConv1x1WrW, SymbolName: 'gcnAsmConv1x1WrW@kd', Language: OpenCL C, LanguageVersion: [ 1, 2 ],
            Attrs:
              { ReqdWorkGroupSize: [ \wg_x, 1, 1 ] }
            CodeProps:
              { KernargSegmentSize: 64, GroupSegmentFixedSize: \lds_size, PrivateSegmentFixedSize: 0, KernargSegmentAlign: 8, WavefrontSize: 64, MaxFlatWorkGroupSize: \wg_x }
            Args:
            - { Name: N       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: C       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: H       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: W       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: K       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: n_groups, Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: unused_0, Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: unused_1, Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: x       , Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: F32, TypeName: 'float*', AddrSpaceQual: Global, AccQual: Default, IsConst: true }
            - { Name: dw      , Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: F32, TypeName: 'float*', AddrSpaceQual: Global, AccQual: Default }
            - { Name: dy      , Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: F32, TypeName: 'float*', AddrSpaceQual: Global, AccQual: Default, IsConst: true }
            - { Name: ret_addr, Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: I32, TypeName: 'int*'  , AddrSpaceQual: Global, AccQual: Default }
          }
    }
    .end_amd_amdgpu_hsa_metadata
  .else
    .error "Unsupported ROCM_METADATA_VERSION"
    .end
  .endif
.endm

workgroup_size_x = n_per_group * 64

.altmacro
.macro METADATA_WRAPPER wg_x, lds_size
    METADATA %\wg_x, %\lds_size
.endm

METADATA_WRAPPER workgroup_size_x, .AUTO_LDS_BYTE_SIZE
    /*******************************************************************************
 *
 * MIT License
 *
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 *******************************************************************************/
/*
 * Convolution Kernel for 5x10 kernel with stride equal to 2 (i.e., -x 10 -y 5 -u 2 -v 2)
 * works on devices compatible with GCN3 ISA, but not XNACK.
 */
/*******************************************************************************
 * 
 * MIT License
 * 
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 * 
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 * 
 *******************************************************************************/

// Wrapper macros for some instructions.
// Macros contain workarounds for some assembler bugs.
// Also these allow for unifying source text when different
// ISA versions require different assembler syntax (mostly mnemonics).

.if ((.option.machine_version_major == 8) || (.option.machine_version_major == 9))
.else
    .error "Only Gfx8 and Gfx9 ISA is supported"
    .end
.endif

.ifndef WORKAROUND_BUG_34765
    .set WORKAROUND_BUG_34765,0
.endif

// Let's use Gfx10-like naming conventions for wrapper macros.
// ADD_NC
.macro _v_add_nc_u32 dst, src0, src1, dpp=
    .if (.option.machine_version_major == 8)
        // None No-Carry instruction in Gfx8, modifies VCC.
        v_add_u32 \dst, vcc, \src0, \src1 \dpp
    .else
        v_add_u32 \dst, \src0, \src1 \dpp
    .endif
.endm

// SUB_NC
.macro _v_sub_nc_u32 dst, src0, src1, dpp=
    .if (.option.machine_version_major == 8)
        // None No-Carry instruction in Gfx8, modifies VCC.
        v_sub_u32 \dst, vcc, \src0, \src1 \dpp
    .else
        v_sub_u32 \dst, \src0, \src1 \dpp
    .endif
.endm

// ADD_CO (gfx8 add)
.macro _v_add_co_u32 dst, co, src0, src1, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
        v_add_u32 \dst, \co, \src0, \src1 \dpp
    .else
        v_add_co_u32 \dst, \co, \src0, \src1 \dpp
    .endif
.endm

// ADD_CO_CI (gfx8 addc)
.macro _v_add_co_ci_u32 dst, co, src0, src1, ci, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
		v_addc_u32 \dst, \co, \src0, \src1, \ci \dpp
    .else
		v_addc_co_u32 \dst, \co, \src0, \src1, \ci \dpp
    .endif
.endm

// SUB_CO (gfx8 sub)
.macro _v_sub_co_u32 dst, co, src0, src1, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
        v_sub_u32 \dst, \co, \src0, \src1 \dpp
    .else
        v_sub_co_u32 \dst, \co, \src0, \src1 \dpp
    .endif
.endm

// SUBREV_CO (gfx8 subrev)
.macro _v_subrev_co_u32 dst, co, src0, src1, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
        v_subrev_u32 \dst, \co, \src0, \src1 \dpp
    .else
        v_subrev_co_u32 \dst, \co, \src0, \src1 \dpp
    .endif
.endm

.macro v_dot2 acc, in, out
    .if (.option.machine_version_major == 9) && (.option.machine_version_minor == 0) && (.option.machine_version_stepping == 6)
        v_dot2_f32_f16 v[\acc], v[\in], v[\out], v[\acc]
    .elseif (.option.machine_version_major == 9) && (.option.machine_version_stepping > 2)
        v_fma_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
        v_fma_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[1,1,0] op_sel_hi:[1,1,0]
    .else
        v_mad_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
        v_mad_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[1,1,0] op_sel_hi:[1,1,0]
    .endif
.endm




.hsa_code_object_version 2,1
.hsa_code_object_isa
.if (.option.machine_version_major != 8) && (.option.machine_version_major != 9)
.error "ERROR: specified target machine not supported"
.endif

///////////////////////////////////////////////////
// ******* global-work and work-group-size
//  work-group-size = [64, 8, 1]
//  global-work = [alignUp(out_w,64), (alignUp(out_h,4)/4)*alignUp(wei_k/2,8), batch_n]
//    * def alignUp(a,b) = ((a + b - 1)/b)*b
//    * def out_w = (inp_w + 2*pad_w + inp_u - wei_w) / inp_u
//    * def out_h = (inp_h + 2*pad_h + inp_v - wei_h) / inp_v
//  NOTE: Each workgroup will process 1x16x4x64(NCHW) tile of output tensor.
//        So there will some loss of performance when output tensor size
//        is not an integer multiple of 1x16x4x64 tile.
///////////////////////////////////////////////////
// ******* changeable configuration parameters
//   inp_w          - input tensor width
//   inp_h          - input tensor height
//   wei_c          - input tensor channels
//   wei_k          - output tensor channels (must be multiple of 2)
//   wei_layout     - weights layout 0:"KCHW" or 1:"CKHW"
//   pad_w          - input padding on left and right
//   pad_h          - input padding on top and bottom
.ifndef inp_w
.error "ERROR: configurable parameter: inp_w must be defined"
.endif
.ifndef inp_h
.error "ERROR: configurable parameter: inp_h must be defined"
.endif
.ifndef wei_c
.error "ERROR: configurable parameter: wei_c must be defined"
.endif
.ifndef wei_k
.error "ERROR: configurable parameter: wei_k must be defined"
.endif
.ifndef wei_layout
.error "ERROR: configurable parameter: wei_layout must be defined"
.endif
.if (wei_k % 2) != 0
.error "ERROR: wei_k must be multiple of 2"
.endif
.ifndef pad_w
.error "ERROR: configurable parameter: pad_w must be defined"
.endif
.ifndef pad_h
.error "ERROR: configurable parameter: pad_h must be defined"
.endif
// ******* build flags
.set use_ds_read_b128, 0
// ******* fixed configuration parameters
.set wei_w       ,  10
.set wei_h       ,   5
.set inp_u       ,   2
.set inp_v       ,   2
// ******* LDS allocation
.set LDS_SIZE    ,4*(64*inp_u-1+(wei_w-1))*(4*inp_v-1+(wei_h-1))+32 // = 6016 bytes
// ******* SGPR allocation
// For used during initialization or as temporary
.set sreg_karg   ,   0   // [2]
.set sreg_group_0,   2   // [1]
.set sreg_group_1,   3   // [1]
.set sreg_group_2,   4   // [1]
.set sreg_tmp0   ,   5   // [1]
.set sreg_tmp1   ,   6   // [1]
.set sreg_tmp2   ,   7   // [1]
.set sreg_iinc   ,   8   // [1]
.set sreg_winc   ,   9   // [1]
.set sreg_inp_addr, 10   // [2]
.set sreg_out_addr, 12   // [2]/[4]
.set sreg_dswr1vcc, 14   // [2]
.set sreg_k       , 16   // [1]
.set sreg_c       , 17   // [1]
.set sreg_dy      , 18   // [1]
.set sreg_oinc    , 19   // [1]
.set sreg_dsrd0vcc, 20   // [2]
.set sreg_dsrd1vcc, 22   // [2]
.set sreg_dspl0vcc, 24   // [2]
.set sreg_dspl1vcc, 26   // [2]
.set sreg_dspr0vcc, 28   // [2]
.set sreg_dspr1vcc, 30   // [2]
.set sreg_pad_val , 32   // [1]
.set sreg_sx      , 33   // [1]
.set sreg_sy      , 34   // [1]
// For use during core-loop and later
.set sreg_wval    ,  0   // [100]
.set sreg_wei_addr,100   // [2]
.set sreg_vcc_resv,102   // [2]
.set SGPR_COUNT   ,108   // COUNT
// ******* VGPR allocation
// For used during initialization
.set vreg_local_0 ,  0   // [1]
.set vreg_local_1 ,  1   // [1]
.set vreg_local_2 ,  2   // [1] unused
.set vreg_tmp0    ,  3   // [1]
.set vreg_tmp1    ,  4   // [1]
.set vreg_tmp2    ,  5   // [1]
.set vreg_tmp3    ,  6   // [1]
.set vreg_lx0     ,  7   // [1]
.set vreg_ly0     ,  8   // [1]
.set vreg_lx1     ,  9   // [1]
.set vreg_ly1     , 10   // [1]
.set vreg_sx0     , 11   // [1]
.set vreg_sy0     , 12   // [1]
.set vreg_sx1     , 13   // [1]
.set vreg_sy1     , 14   // [1]
// For use during core-loop and later
.set vreg_ival    ,  0   // [8]
.set vreg_oval    ,  8   // [8]
.set vreg_iinc0   , 16   // [1]
.set vreg_iinc1   , 17   // [1]
.set vreg_inp_dswr0,18   // [1]
.set vreg_inp_dswr1,19   // [1]
.set vreg_inp_dsrd0,20   // [1]
.set vreg_dx      , 21   // [1]
.set vreg_save    , 22   // [1]
.set VGPR_COUNT   , 24   // COUNT
// Lane# of saved sreg_* in vreg_save
.set lane_vreg_save_wr1vcc0   ,  0
.set lane_vreg_save_wr1vcc1   ,  1
.set lane_vreg_save_c         ,  2
.set lane_vreg_save_k         ,  3
.set lane_vreg_save_dy        ,  4
.set lane_vreg_save_inp_addr0 ,  5
.set lane_vreg_save_inp_addr1 ,  6
.set lane_vreg_save_out_addr0 ,  7
.set lane_vreg_save_out_addr1 ,  8
.set lane_vreg_save_pad_val   ,  9
.set lane_vreg_save_rd0vcc0   , 10
.set lane_vreg_save_rd0vcc1   , 11
.set lane_vreg_save_rd1vcc0   , 12
.set lane_vreg_save_rd1vcc1   , 13
.set lane_vreg_save_pl0vcc0   , 14
.set lane_vreg_save_pl0vcc1   , 15
.set lane_vreg_save_pr0vcc0   , 16
.set lane_vreg_save_pr0vcc1   , 17
.set lane_vreg_save_pl1vcc0   , 18
.set lane_vreg_save_pl1vcc1   , 19
.set lane_vreg_save_pr1vcc0   , 20
.set lane_vreg_save_pr1vcc1   , 21
// ******* derived constants
.set out_w       ,(inp_w + 2*pad_w + inp_u - wei_w) / inp_u
.set out_h       ,(inp_h + 2*pad_h + inp_v - wei_h) / inp_v
.set inp_stride_y,(inp_w * 4)
.set inp_stride_c,(inp_h * inp_stride_y)
.set inp_stride_n,(wei_c * inp_stride_c)
.set out_stride_y,(out_w * 4)
.set out_stride_k,(out_h * out_stride_y)
.set out_stride_n,(wei_k * out_stride_k)
.if wei_layout == 0 // KCHW
.set wei_stride_c,(wei_h * wei_w * 4)
.set wei_stride_k,(wei_c * wei_stride_c)
.elseif wei_layout == 1 // CKHW
.set wei_stride_k,(wei_h * wei_w * 4)
.set wei_stride_c,(wei_k * wei_stride_k)
.else
.error "ERROR: wei_layout should be 0 (for:KCHW) or 1 (for:CKHW)"
.endif
.macro .bitcount, n, bits
  .if (1 << \bits) < \n
    .set \bits, \bits + 1
    .bitcount wei_k, wei_k_bits
  .endif
.endm
.set wei_k_bits  , 0
.bitcount wei_k  , wei_k_bits
.set wei_k_mask  , ((1 << wei_k_bits) - 1)
// ******* derived flags
.set padding_enabled     , ((pad_w != 0) || (pad_h != 0))
.set dspl0_enabled       ,  (pad_w % 2) == 1
.set dspl1_enabled       ,  (pad_w % 2) == 1
.set dspr0_enabled       ,  (pad_w > 0) && (((inp_w + pad_w) % 2) == 1) && (((inp_w + pad_w) % 128) >= 8)
.set dspr1_enabled       ,  (pad_w > 0) && (((inp_w + pad_w) % 2) == 1)
// ******* macros for ds_read_b128
.macro macro_ds_read_b128 dest, addr, offset=0
  .short 0x0+\offset, 0xd9fe
  .byte 0x0+\addr, 0x0, 0x0, 0x0+\dest
.endm

///////////////////////////////////////////////////
// ******* text section of the kernels
///////////////////////////////////////////////////
.text
.p2align 8
.global conv5x10u2v2f1
.type conv5x10u2v2f1, @function
.amdgpu_hsa_kernel conv5x10u2v2f1
conv5x10u2v2f1:

    .amd_kernel_code_t
        amd_machine_version_major = .option.machine_version_major
        amd_machine_version_minor = .option.machine_version_minor
        amd_machine_version_stepping = .option.machine_version_stepping
        is_ptr64 = 1
        float_mode = 192
        user_sgpr_count = 2
        is_xnack_enabled = 0
        enable_sgpr_workgroup_id_x = 1
        enable_sgpr_workgroup_id_y = 1
        enable_sgpr_workgroup_id_z = 1
        enable_vgpr_workitem_id = 1
        enable_sgpr_kernarg_segment_ptr = 1
        workitem_vgpr_count = VGPR_COUNT
        wavefront_sgpr_count = SGPR_COUNT
        workgroup_group_segment_byte_size = LDS_SIZE
        kernarg_segment_byte_size = 56
        granulated_workitem_vgpr_count = (VGPR_COUNT-1)/4
        granulated_wavefront_sgpr_count = (SGPR_COUNT-1)/8
    .end_amd_kernel_code_t

    //////////////////////////////////////////////////////////////////////////////
    // initialization
    //  - work-items:
    //      work-group-size = [64, 8, 1]
    //      global-work = [alignUp(out_w,64), (alignUp(out_h,4)/4)*alignUp(wei_k/2,8), batch_n]
    //      work-item relation to output buffer:
    //        dx =  global-work[0]
    //        dy = (global-work[1] >> (wei_k_bits-4)) * 4
    //        k  = (global-work[1] << 1) & wei_k_mask
    //        n  =  global-work[2]
    //      calculation:
    //        dx =  group_id(0) * 64 + local_id(0)
    //        dy = (group_id(1) >> (wei_k_bits-4)) * 4
    //        k  =((group_id(1) << 3) + local_id(1))*2 & wei_k_mask
    //        n  =  group_id(2)
    // - calculate sreg_wei_addr for current wave
    //      sreg_wei_addr += min(k,wei_k-1) * wei_stride_k
    //  - calculate sreg_out_addr for current wave
    //      sreg_out_addr += n * out_stride_n + k * out_stride_k + dy * out_stride_y
    //  - calculate sreg_inp_addr for current wave
    //      sreg_inp_addr += n * inp_stride_n
    //  - calculate registers for managing input in LDS
    //      lx0 = local_id(0) * 2
    //      ly0 = local_id(1)
    //      lx1 = (local_id(1) < 3) ? ((local_id(0)  & 3)*2 + 128) ?  lx0
    //      ly1 = (local_id(1) < 3) ?  (local_id(0) >> 2)          : (ly0 + 8)
    //      vreg_inp_dswr0 = lx0 * 4 + ly0 * 136 * 4
    //      vreg_inp_dswr1 = lx1 * 4 + ly1 * 136 * 4
    //      vreg_inp_dsrd0 = lx0 * 4
    //      sx  = 2 * group_id(0) * 64
    //      sy  = 2 * sreg_dy
    //      sx0 = sx + lx0 - pad_w
    //      sy0 = sy + ly0 - pad_h
    //      sx1 = sx + lx1 - pad_w
    //      sy1 = sy + ly1 - pad_h
    //      vreg_iinc0 = sy0 * inp_stride_y + max(0, sx0) * 4
    //      vreg_iinc1 = sy1 * inp_stride_y + max(0, sx1) * 4
    //      dsr_right_pos = inp_w + (inp_w & 1) - (pad_w & inp_w & 1)
    //      sreg_dsrd0vcc = (sx0 >= -1 && sx0 < dsr_right_pos && sy0 >= 0 && sy0 < inp_h)
    //      sreg_dsrd1vcc = (sx1 >= -1 && sx1 < dsr_right_pos && sy1 >= 0 && sy1 < inp_h) && (ly1 < 11)
    //      sreg_dswr1vcc = (ly1 < 11)
    //      dsp_right_pos = inp_w - (inp_w & 1) - (pad_w & 1)
    //      sreg_dspl0vcc = (sx0 == -1)
    //      sreg_dspr0vcc = (sx0 == dsp_right_pos)
    //      sreg_dspl1vcc = (sx1 == -1)
    //      sreg_dspr0vcc = (sx1 == dsp_right_pos)
    //  - save sreg values in vreg_save
    //  - initialize sreg_c and output values
    //////////////////////////////////////////////////////////////////////////////
    s_mov_b32 m0, LDS_SIZE
    // load for parameters
    s_load_dwordx2 s[sreg_inp_addr:sreg_inp_addr+1], s[sreg_karg:sreg_karg+1], 0x00
    s_load_dwordx2 s[sreg_wei_addr:sreg_wei_addr+1], s[sreg_karg:sreg_karg+1], 0x08
    s_load_dwordx2 s[sreg_out_addr:sreg_out_addr+1], s[sreg_karg:sreg_karg+1], 0x10
.if padding_enabled
    s_load_dword   s[sreg_pad_val                 ], s[sreg_karg:sreg_karg+1], 0x18
.endif
    // compute: sreg_dx, sreg_dy, sreg_k
    s_lshl_b32 s[sreg_tmp0], s[sreg_group_0], 6
   _v_add_co_u32 v[vreg_dx], vcc, s[sreg_tmp0], v[vreg_local_0]
    s_lshr_b32 s[sreg_dy], s[sreg_group_1], wei_k_bits-4
    s_lshl_b32 s[sreg_dy], s[sreg_dy], 2
    v_readfirstlane_b32 s[sreg_k], v[vreg_local_1]
    s_lshl_b32 s[sreg_tmp0], s[sreg_group_1], 3
    s_add_u32  s[sreg_k], s[sreg_k], s[sreg_tmp0]
    s_lshl_b32 s[sreg_k], s[sreg_k], 1
    s_and_b32  s[sreg_k], s[sreg_k], wei_k_mask
    // compute: sreg_winc = min(k,wei_k-1) * wei_stride_k
    s_min_i32  s[sreg_winc], s[sreg_k], wei_k-1
    s_mul_i32  s[sreg_winc], s[sreg_winc], wei_stride_k
    // compute: sreg_oinc = group_id(2) * out_stride_n + k * out_stride_k + dy * out_stride_y
    s_mul_i32  s[sreg_oinc], s[sreg_group_2], out_stride_n
    s_mul_i32  s[sreg_tmp1], s[sreg_k], out_stride_k
    s_add_u32  s[sreg_oinc], s[sreg_oinc], s[sreg_tmp1]
    s_mul_i32  s[sreg_tmp1], s[sreg_dy], out_stride_y
    s_add_u32  s[sreg_oinc], s[sreg_oinc], s[sreg_tmp1]
    // compute: registers for transfering input into LDS
    v_lshlrev_b32 v[vreg_lx0], 1, v[vreg_local_0]
    v_mov_b32     v[vreg_ly0], v[vreg_local_1]
    v_and_b32     v[vreg_lx1], 3, v[vreg_local_0]
    v_lshlrev_b32 v[vreg_lx1], 1, v[vreg_lx1]
   _v_add_co_u32  v[vreg_lx1], vcc, 128, v[vreg_lx1]
    v_lshrrev_b32 v[vreg_ly1], 2, v[vreg_local_0]
   _v_add_co_u32  v[vreg_tmp0], vcc, 8, v[vreg_ly0]
    v_cmp_eq_u32  vcc, 3, v[vreg_local_1]
    v_cndmask_b32 v[vreg_lx1], v[vreg_lx0], v[vreg_lx1], vcc
    v_cndmask_b32 v[vreg_ly1], v[vreg_tmp0], v[vreg_ly1], vcc
    v_lshlrev_b32 v[vreg_inp_dsrd0], 2, v[vreg_lx0]
    s_movk_i32    s[sreg_tmp0], 136*4
    v_lshlrev_b32 v[vreg_tmp1], 2, v[vreg_lx1]
    v_mad_u32_u24 v[vreg_inp_dswr0], v[vreg_ly0], s[sreg_tmp0], v[vreg_inp_dsrd0]
    v_mad_u32_u24 v[vreg_inp_dswr1], v[vreg_ly1], s[sreg_tmp0], v[vreg_tmp1]
    s_lshl_b32    s[sreg_sx], s[sreg_group_0], 7
    s_lshl_b32    s[sreg_sy], s[sreg_dy], 1
    s_sub_u32     s[sreg_sx], s[sreg_sx], 0+pad_w
    s_sub_u32     s[sreg_sy], s[sreg_sy], 0+pad_h
   _v_add_co_u32  v[vreg_sx0], vcc, s[sreg_sx], v[vreg_lx0]
   _v_add_co_u32  v[vreg_sy0], vcc, s[sreg_sy], v[vreg_ly0]
   _v_add_co_u32  v[vreg_sx1], vcc, s[sreg_sx], v[vreg_lx1]
   _v_add_co_u32  v[vreg_sy1], vcc, s[sreg_sy], v[vreg_ly1]
    v_max_i32     v[vreg_iinc0], 0, v[vreg_sx0]
    v_max_i32     v[vreg_iinc1], 0, v[vreg_sx1]
    v_lshlrev_b32 v[vreg_iinc0], 2, v[vreg_iinc0]
    v_lshlrev_b32 v[vreg_iinc1], 2, v[vreg_iinc1]
    v_mov_b32     v[vreg_tmp0], 0+inp_stride_y
    v_mad_u32_u24 v[vreg_iinc0], v[vreg_sy0], v[vreg_tmp0], v[vreg_iinc0]
    v_mad_u32_u24 v[vreg_iinc1], v[vreg_sy1], v[vreg_tmp0], v[vreg_iinc1]
.if padding_enabled
    v_mov_b32     v[vreg_tmp0], 0+inp_w+(inp_w & 1)-(inp_w & pad_w & 1)
    v_mov_b32     v[vreg_tmp1], 0+inp_h
    v_cmp_le_i32  vcc, -1, v[vreg_sx0]
    s_mov_b64     s[sreg_dsrd0vcc:sreg_dsrd0vcc+1], vcc
    v_cmp_gt_i32  vcc, v[vreg_tmp0], v[vreg_sx0]
    s_and_b64     s[sreg_dsrd0vcc:sreg_dsrd0vcc+1], s[sreg_dsrd0vcc:sreg_dsrd0vcc+1], vcc
    v_cmp_le_i32  vcc, 0, v[vreg_sy0]
    s_and_b64     s[sreg_dsrd0vcc:sreg_dsrd0vcc+1], s[sreg_dsrd0vcc:sreg_dsrd0vcc+1], vcc
    v_cmp_gt_i32  vcc, v[vreg_tmp1], v[vreg_sy0]
    s_and_b64     s[sreg_dsrd0vcc:sreg_dsrd0vcc+1], s[sreg_dsrd0vcc:sreg_dsrd0vcc+1], vcc
.endif
    v_cmp_gt_i32  vcc, 11, v[vreg_ly1]
    s_mov_b64     s[sreg_dswr1vcc:sreg_dswr1vcc+1], vcc
.if padding_enabled
    v_cmp_le_i32  vcc, -1, v[vreg_sx1]
    s_and_b64     s[sreg_dsrd1vcc:sreg_dsrd1vcc+1], s[sreg_dswr1vcc:sreg_dswr1vcc+1], vcc
    v_cmp_gt_i32  vcc, v[vreg_tmp0], v[vreg_sx1]
    s_and_b64     s[sreg_dsrd1vcc:sreg_dsrd1vcc+1], s[sreg_dsrd1vcc:sreg_dsrd1vcc+1], vcc
    v_cmp_le_i32  vcc, 0, v[vreg_sy1]
    s_and_b64     s[sreg_dsrd1vcc:sreg_dsrd1vcc+1], s[sreg_dsrd1vcc:sreg_dsrd1vcc+1], vcc
    v_cmp_gt_i32  vcc, v[vreg_tmp1], v[vreg_sy1]
    s_and_b64     s[sreg_dsrd1vcc:sreg_dsrd1vcc+1], s[sreg_dsrd1vcc:sreg_dsrd1vcc+1], vcc
.if dspr0_enabled || dspr1_enabled
    v_mov_b32     v[vreg_tmp0], 0+inp_w-(inp_w & 1)-(pad_w & 1)
.endif
.if dspl0_enabled
    v_cmp_eq_i32  vcc, -1, v[vreg_sx0]
    s_mov_b64     s[sreg_dspl0vcc:sreg_dspl0vcc+1], vcc
.endif
.if dspr0_enabled
    v_cmp_eq_i32  vcc, v[vreg_tmp0], v[vreg_sx0]
    s_mov_b64     s[sreg_dspr0vcc:sreg_dspr0vcc+1], vcc
.endif
.if dspl1_enabled
    v_cmp_eq_i32  vcc, -1, v[vreg_sx1]
    s_mov_b64     s[sreg_dspl1vcc:sreg_dspl1vcc+1], vcc
.endif
.if dspr1_enabled
    v_cmp_eq_i32  vcc, v[vreg_tmp0], v[vreg_sx1]
    s_mov_b64     s[sreg_dspr1vcc:sreg_dspr1vcc+1], vcc
.endif
.endif
    // wait for load completion
    s_waitcnt lgkmcnt(0)
    // update address registers
    //   sreg_inp_addr += group_id(2) * inp_stride_n
    //   sreg_wei_addr += sreg_winc
    //   sreg_out_addr += sreg_oinc
    s_add_u32  s[sreg_wei_addr], s[sreg_wei_addr], s[sreg_winc]
    s_addc_u32 s[sreg_wei_addr+1], s[sreg_wei_addr+1], 0
    s_add_u32  s[sreg_out_addr], s[sreg_out_addr], s[sreg_oinc]
    s_addc_u32 s[sreg_out_addr+1], s[sreg_out_addr+1], 0
    s_mul_i32  s[sreg_tmp0], s[sreg_group_2], inp_stride_n
    s_add_u32  s[sreg_inp_addr], s[sreg_inp_addr], s[sreg_tmp0]
    s_addc_u32 s[sreg_inp_addr+1], s[sreg_inp_addr+1], 0
    // initialize output values and channel count
    s_movk_i32 s[sreg_c], 0+wei_c
    v_mov_b32 v[vreg_oval+0], 0
    v_mov_b32 v[vreg_oval+1], 0
    v_mov_b32 v[vreg_oval+2], 0
    v_mov_b32 v[vreg_oval+3], 0
    v_mov_b32 v[vreg_oval+4], 0
    v_mov_b32 v[vreg_oval+5], 0
    v_mov_b32 v[vreg_oval+6], 0
    v_mov_b32 v[vreg_oval+7], 0
    // save sreg_c, sreg_k, sreg_dy, sreg_inp/out_addr, sreg_*vcc:sreg_*vcc+1, sreg_pad_val into vreg_save
    v_writelane_b32 v[vreg_save], s[sreg_dswr1vcc], 0+lane_vreg_save_wr1vcc0
    v_writelane_b32 v[vreg_save], s[sreg_dswr1vcc+1], 0+lane_vreg_save_wr1vcc1
    v_writelane_b32 v[vreg_save], s[sreg_c], 0+lane_vreg_save_c
    v_writelane_b32 v[vreg_save], s[sreg_k], 0+lane_vreg_save_k
    v_writelane_b32 v[vreg_save], s[sreg_dy], 0+lane_vreg_save_dy
    v_writelane_b32 v[vreg_save], s[sreg_inp_addr+0], 0+lane_vreg_save_inp_addr0
    v_writelane_b32 v[vreg_save], s[sreg_inp_addr+1], 0+lane_vreg_save_inp_addr1
    v_writelane_b32 v[vreg_save], s[sreg_out_addr+0], 0+lane_vreg_save_out_addr0
    v_writelane_b32 v[vreg_save], s[sreg_out_addr+1], 0+lane_vreg_save_out_addr1
.if padding_enabled
    v_writelane_b32 v[vreg_save], s[sreg_pad_val], 0+lane_vreg_save_pad_val
    v_writelane_b32 v[vreg_save], s[sreg_dsrd0vcc+0], 0+lane_vreg_save_rd0vcc0
    v_writelane_b32 v[vreg_save], s[sreg_dsrd0vcc+1], 0+lane_vreg_save_rd0vcc1
    v_writelane_b32 v[vreg_save], s[sreg_dsrd1vcc+0], 0+lane_vreg_save_rd1vcc0
    v_writelane_b32 v[vreg_save], s[sreg_dsrd1vcc+1], 0+lane_vreg_save_rd1vcc1
.if dspl0_enabled
    v_writelane_b32 v[vreg_save], s[sreg_dspl0vcc+0], 0+lane_vreg_save_pl0vcc0
    v_writelane_b32 v[vreg_save], s[sreg_dspl0vcc+1], 0+lane_vreg_save_pl0vcc1
.endif
.if dspl1_enabled
    v_writelane_b32 v[vreg_save], s[sreg_dspl1vcc+0], 0+lane_vreg_save_pl1vcc0
    v_writelane_b32 v[vreg_save], s[sreg_dspl1vcc+1], 0+lane_vreg_save_pl1vcc1
.endif
.if dspr0_enabled
    v_writelane_b32 v[vreg_save], s[sreg_dspr0vcc+0], 0+lane_vreg_save_pr0vcc0
    v_writelane_b32 v[vreg_save], s[sreg_dspr0vcc+1], 0+lane_vreg_save_pr0vcc1
.endif
.if dspr1_enabled
    v_writelane_b32 v[vreg_save], s[sreg_dspr1vcc+0], 0+lane_vreg_save_pr1vcc0
    v_writelane_b32 v[vreg_save], s[sreg_dspr1vcc+1], 0+lane_vreg_save_pr1vcc1
.endif
.endif

    //////////////////////////////////////////////////////////////////////////////
    // loop though all channels:
    // registers with valid data from initialization
    //  - s[sreg_wei_addr:sreg_wei_addr+1]
    //  - v[vreg_save]
    //  - v[vreg_dx]
    //  - v[vreg_iinc0]
    //  - v[vreg_iinc1]
    //  - v[vreg_dswr0]
    //  - v[vreg_dswr1]
    //  - v[vreg_dsrd0]
    //  - v[vreg_oval:vreg_oval+7]
    // temporary registers used inside this loop:
    //  - s[sreg_wval:sreg_wval+99]
    //  - v[vreg_ival:vreg_ival+7]
    //////////////////////////////////////////////////////////////////////////////
loop_channel:
    // load input row into LDS
.if padding_enabled
    v_readlane_b32 s[sreg_dsrd0vcc+0], v[vreg_save], 0+lane_vreg_save_rd0vcc0
    v_readlane_b32 s[sreg_dsrd0vcc+1], v[vreg_save], 0+lane_vreg_save_rd0vcc1
    v_readlane_b32 s[sreg_dsrd1vcc+0], v[vreg_save], 0+lane_vreg_save_rd1vcc0
    v_readlane_b32 s[sreg_dsrd1vcc+1], v[vreg_save], 0+lane_vreg_save_rd1vcc1
.endif
    v_readlane_b32 s[sreg_wval+0], v[vreg_save], 0+lane_vreg_save_inp_addr0
    v_readlane_b32 s[sreg_wval+1], v[vreg_save], 0+lane_vreg_save_inp_addr1
    v_readlane_b32 s[sreg_dswr1vcc+0], v[vreg_save], 0+lane_vreg_save_wr1vcc0
    v_readlane_b32 s[sreg_dswr1vcc+1], v[vreg_save], 0+lane_vreg_save_wr1vcc1
    s_mov_b32      s[sreg_wval+2], 0+inp_stride_n
    s_mov_b32      s[sreg_wval+3], 0x00020000
.if padding_enabled
    v_readlane_b32 s[sreg_pad_val], v[vreg_save], 0+lane_vreg_save_pad_val
    v_mov_b32 v[vreg_ival+0], s[sreg_pad_val]
    v_mov_b32 v[vreg_ival+1], s[sreg_pad_val]
    v_mov_b32 v[vreg_ival+2], s[sreg_pad_val]
    v_mov_b32 v[vreg_ival+3], s[sreg_pad_val]
    s_mov_b64 exec, s[sreg_dsrd0vcc:sreg_dsrd0vcc+1]
.else
    s_nop 0
.endif
    buffer_load_dwordx2 v[vreg_ival+0:vreg_ival+1], v[vreg_iinc0], s[sreg_wval+0:sreg_wval+3], 0 offen offset:0
.if padding_enabled
    s_mov_b64 exec, s[sreg_dsrd1vcc:sreg_dsrd1vcc+1]
.else
    s_mov_b64 exec, s[sreg_dswr1vcc:sreg_dswr1vcc+1]
.endif
    buffer_load_dwordx2 v[vreg_ival+2:vreg_ival+3], v[vreg_iinc1], s[sreg_wval+0:sreg_wval+3], 0 offen offset:0
    s_mov_b64 exec, -1
    v_mov_b32  v[vreg_ival+4], 0+inp_stride_c
   _v_add_co_u32 v[vreg_iinc0], vcc, v[vreg_iinc0], v[vreg_ival+4]
   _v_add_co_u32 v[vreg_iinc1], vcc, v[vreg_iinc1], v[vreg_ival+4]
.if padding_enabled
.if dspl0_enabled
    v_readlane_b32 s[sreg_dspl0vcc+0], v[vreg_save], 0+lane_vreg_save_pl0vcc0
    v_readlane_b32 s[sreg_dspl0vcc+1], v[vreg_save], 0+lane_vreg_save_pl0vcc1
.endif
.if dspr0_enabled
    v_readlane_b32 s[sreg_dspr0vcc+0], v[vreg_save], 0+lane_vreg_save_pr0vcc0
    v_readlane_b32 s[sreg_dspr0vcc+1], v[vreg_save], 0+lane_vreg_save_pr0vcc1
.endif
.if dspl1_enabled
    v_readlane_b32 s[sreg_dspl1vcc+0], v[vreg_save], 0+lane_vreg_save_pl1vcc0
    v_readlane_b32 s[sreg_dspl1vcc+1], v[vreg_save], 0+lane_vreg_save_pl1vcc1
.endif
.if dspr1_enabled
    v_readlane_b32 s[sreg_dspr1vcc+0], v[vreg_save], 0+lane_vreg_save_pr1vcc0
    v_readlane_b32 s[sreg_dspr1vcc+1], v[vreg_save], 0+lane_vreg_save_pr1vcc1
.endif
.endif
    s_waitcnt lgkmcnt(0) vmcnt(0)
.if padding_enabled
.if dspl0_enabled
    s_mov_b64 exec, s[sreg_dspl0vcc:sreg_dspl0vcc+1]
    v_mov_b32 v[vreg_ival+1], v[vreg_ival+0]
    v_mov_b32 v[vreg_ival+0], s[sreg_pad_val]
.endif
.if dspr0_enabled
    s_mov_b64 exec, s[sreg_dspr0vcc:sreg_dspr0vcc+1]
    v_mov_b32 v[vreg_ival+1], s[sreg_pad_val]
.endif
.if dspl1_enabled
    s_mov_b64 exec, s[sreg_dspl1vcc:sreg_dspl1vcc+1]
    v_mov_b32 v[vreg_ival+3], v[vreg_ival+2]
    v_mov_b32 v[vreg_ival+2], s[sreg_pad_val]
.endif
.if dspr1_enabled
    s_mov_b64 exec, s[sreg_dspr1vcc:sreg_dspr1vcc+1]
    v_mov_b32 v[vreg_ival+3], s[sreg_pad_val]
.endif
.if dspl0_enabled || dspr0_enabled || dspl1_enabled || dspr1_enabled
    s_mov_b64 exec, -1
.endif
.endif
    s_barrier
    ds_write_b64 v[vreg_inp_dswr0], v[vreg_ival+0:vreg_ival+1]
    s_mov_b64 exec, s[sreg_dswr1vcc:sreg_dswr1vcc+1]
    ds_write_b64 v[vreg_inp_dswr1], v[vreg_ival+2:vreg_ival+3]
    s_mov_b64 exec, -1
    s_waitcnt lgkmcnt(0)
    s_barrier
    // load channel weights and update sreg_wei_addr for next loop iteration
    s_load_dwordx16 s[sreg_wval   :sreg_wval+15], s[sreg_wei_addr:sreg_wei_addr+1], 0
    s_load_dwordx16 s[sreg_wval+16:sreg_wval+31], s[sreg_wei_addr:sreg_wei_addr+1], 4*16
    s_load_dwordx16 s[sreg_wval+32:sreg_wval+47], s[sreg_wei_addr:sreg_wei_addr+1], 4*32
    s_load_dwordx2  s[sreg_wval+96:sreg_wval+97], s[sreg_wei_addr:sreg_wei_addr+1], 4*48
    s_add_u32  s[sreg_wei_addr], s[sreg_wei_addr], wei_stride_k
    s_addc_u32 s[sreg_wei_addr+1], s[sreg_wei_addr+1], 0
    s_load_dwordx16 s[sreg_wval+48:sreg_wval+63], s[sreg_wei_addr:sreg_wei_addr+1], 0
    s_load_dwordx16 s[sreg_wval+64:sreg_wval+79], s[sreg_wei_addr:sreg_wei_addr+1], 4*16
    s_load_dwordx16 s[sreg_wval+80:sreg_wval+95], s[sreg_wei_addr:sreg_wei_addr+1], 4*32
    s_load_dwordx2  s[sreg_wval+98:sreg_wval+99], s[sreg_wei_addr:sreg_wei_addr+1], 4*48
.if wei_layout == 0 // KCHW
    s_add_u32  s[sreg_wei_addr], s[sreg_wei_addr], wei_stride_c-wei_stride_k
    s_addc_u32 s[sreg_wei_addr+1], s[sreg_wei_addr+1], -1
.else // CKHW
    s_add_u32  s[sreg_wei_addr], s[sreg_wei_addr], wei_stride_c-wei_stride_k
    s_addc_u32 s[sreg_wei_addr+1], s[sreg_wei_addr+1], 0
.endif
    s_waitcnt lgkmcnt(0) vmcnt(0)

    // compute 2D conv
    .if use_ds_read_b128
    macro_ds_read_b128 vreg_ival+0, vreg_inp_dsrd0 4*(0*136+0)
    macro_ds_read_b128 vreg_ival+4, vreg_inp_dsrd0 4*(0*136+4)
    s_waitcnt lgkmcnt(1)
    .else
    ds_read_b64 v[vreg_ival+0:vreg_ival+1], v[vreg_inp_dsrd0] offset:4*(0*136+0)
    ds_read_b64 v[vreg_ival+2:vreg_ival+3], v[vreg_inp_dsrd0] offset:4*(0*136+2)
    ds_read_b64 v[vreg_ival+4:vreg_ival+5], v[vreg_inp_dsrd0] offset:4*(0*136+4)
    ds_read_b64 v[vreg_ival+6:vreg_ival+7], v[vreg_inp_dsrd0] offset:4*(0*136+6)
    s_waitcnt lgkmcnt(2)
    .endif
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+0], s[sreg_wval+0*10+0]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+1], s[sreg_wval+0*10+1]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+2], s[sreg_wval+0*10+2]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+3], s[sreg_wval+0*10+3]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+0], s[sreg_wval+0*10+0+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+1], s[sreg_wval+0*10+1+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+2], s[sreg_wval+0*10+2+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+3], s[sreg_wval+0*10+3+48]
    ds_read_b64 v[vreg_ival+0:vreg_ival+1], v[vreg_inp_dsrd0] offset:4*(0*136+8)
    ds_read_b64 v[vreg_ival+2:vreg_ival+3], v[vreg_inp_dsrd0] offset:4*(1*136+0)
    s_waitcnt lgkmcnt(2)
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+4], s[sreg_wval+0*10+4]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+5], s[sreg_wval+0*10+5]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+6], s[sreg_wval+0*10+6]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+7], s[sreg_wval+0*10+7]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+4], s[sreg_wval+0*10+4+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+5], s[sreg_wval+0*10+5+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+6], s[sreg_wval+0*10+6+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+7], s[sreg_wval+0*10+7+48]
    .if use_ds_read_b128
    macro_ds_read_b128 vreg_ival+4, vreg_inp_dsrd0, 4*(1*136+2)
    s_waitcnt lgkmcnt(1)
    .else
    ds_read_b64 v[vreg_ival+4:vreg_ival+5], v[vreg_inp_dsrd0] offset:4*(1*136+2)
    ds_read_b64 v[vreg_ival+6:vreg_ival+7], v[vreg_inp_dsrd0] offset:4*(1*136+4)
    s_waitcnt lgkmcnt(2)
    .endif
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+0], s[sreg_wval+0*10+8]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+1], s[sreg_wval+0*10+9]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+2], s[sreg_wval+1*10+0]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+3], s[sreg_wval+1*10+1]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+0], s[sreg_wval+0*10+8+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+1], s[sreg_wval+0*10+9+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+2], s[sreg_wval+1*10+0+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+3], s[sreg_wval+1*10+1+48]
    .if use_ds_read_b128
    macro_ds_read_b128 vreg_ival+0, vreg_inp_dsrd0, 4*(1*136+6)
    s_waitcnt lgkmcnt(1)
    .else
    ds_read_b64 v[vreg_ival+0:vreg_ival+1], v[vreg_inp_dsrd0] offset:4*(1*136+6)
    ds_read_b64 v[vreg_ival+2:vreg_ival+3], v[vreg_inp_dsrd0] offset:4*(1*136+8)
    s_waitcnt lgkmcnt(2)
    .endif
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+4], s[sreg_wval+1*10+2]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+5], s[sreg_wval+1*10+3]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+6], s[sreg_wval+1*10+4]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+7], s[sreg_wval+1*10+5]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+4], s[sreg_wval+1*10+2+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+5], s[sreg_wval+1*10+3+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+6], s[sreg_wval+1*10+4+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+7], s[sreg_wval+1*10+5+48]
    .if use_ds_read_b128
    macro_ds_read_b128 vreg_ival+4, vreg_inp_dsrd0, 4*(2*136+0)
    s_waitcnt lgkmcnt(1)
    .else
    ds_read_b64 v[vreg_ival+4:vreg_ival+5], v[vreg_inp_dsrd0] offset:4*(2*136+0)
    ds_read_b64 v[vreg_ival+6:vreg_ival+7], v[vreg_inp_dsrd0] offset:4*(2*136+2)
    s_waitcnt lgkmcnt(2)
    .endif
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+0], s[sreg_wval+1*10+6]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+1], s[sreg_wval+1*10+7]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+2], s[sreg_wval+1*10+8]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+3], s[sreg_wval+1*10+9]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+0], s[sreg_wval+1*10+6+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+1], s[sreg_wval+1*10+7+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+2], s[sreg_wval+1*10+8+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+3], s[sreg_wval+1*10+9+48]
    .if use_ds_read_b128
    macro_ds_read_b128 vreg_ival+0, vreg_inp_dsrd0, 4*(2*136+4)
    s_waitcnt lgkmcnt(1)
    .else
    ds_read_b64 v[vreg_ival+0:vreg_ival+1], v[vreg_inp_dsrd0] offset:4*(2*136+4)
    ds_read_b64 v[vreg_ival+2:vreg_ival+3], v[vreg_inp_dsrd0] offset:4*(2*136+6)
    s_waitcnt lgkmcnt(2)
    .endif
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+4], s[sreg_wval+2*10+0]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+5], s[sreg_wval+2*10+1]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+6], s[sreg_wval+2*10+2]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+7], s[sreg_wval+2*10+3]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+4], s[sreg_wval+0*10+0]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+5], s[sreg_wval+0*10+1]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+6], s[sreg_wval+0*10+2]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+7], s[sreg_wval+0*10+3]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+4], s[sreg_wval+2*10+0+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+5], s[sreg_wval+2*10+1+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+6], s[sreg_wval+2*10+2+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+7], s[sreg_wval+2*10+3+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+4], s[sreg_wval+0*10+0+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+5], s[sreg_wval+0*10+1+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+6], s[sreg_wval+0*10+2+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+7], s[sreg_wval+0*10+3+48]
    ds_read_b64 v[vreg_ival+4:vreg_ival+5], v[vreg_inp_dsrd0] offset:4*(2*136+8)
    ds_read_b64 v[vreg_ival+6:vreg_ival+7], v[vreg_inp_dsrd0] offset:4*(3*136+0)
    s_waitcnt lgkmcnt(2)
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+0], s[sreg_wval+2*10+4]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+1], s[sreg_wval+2*10+5]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+2], s[sreg_wval+2*10+6]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+3], s[sreg_wval+2*10+7]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+0], s[sreg_wval+0*10+4]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+1], s[sreg_wval+0*10+5]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+2], s[sreg_wval+0*10+6]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+3], s[sreg_wval+0*10+7]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+0], s[sreg_wval+2*10+4+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+1], s[sreg_wval+2*10+5+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+2], s[sreg_wval+2*10+6+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+3], s[sreg_wval+2*10+7+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+0], s[sreg_wval+0*10+4+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+1], s[sreg_wval+0*10+5+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+2], s[sreg_wval+0*10+6+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+3], s[sreg_wval+0*10+7+48]
    .if use_ds_read_b128
    macro_ds_read_b128 vreg_ival+0, vreg_inp_dsrd0, 4*(3*136+2)
    s_waitcnt lgkmcnt(1)
    .else
    ds_read_b64 v[vreg_ival+0:vreg_ival+1], v[vreg_inp_dsrd0] offset:4*(3*136+2)
    ds_read_b64 v[vreg_ival+2:vreg_ival+3], v[vreg_inp_dsrd0] offset:4*(3*136+4)
    s_waitcnt lgkmcnt(2)
    .endif
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+4], s[sreg_wval+2*10+8]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+5], s[sreg_wval+2*10+9]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+6], s[sreg_wval+3*10+0]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+7], s[sreg_wval+3*10+1]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+4], s[sreg_wval+0*10+8]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+5], s[sreg_wval+0*10+9]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+6], s[sreg_wval+1*10+0]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+7], s[sreg_wval+1*10+1]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+4], s[sreg_wval+2*10+8+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+5], s[sreg_wval+2*10+9+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+6], s[sreg_wval+3*10+0+48]
    v_mac_f32 v[vreg_oval+4], v[vreg_ival+7], s[sreg_wval+3*10+1+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+4], s[sreg_wval+0*10+8+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+5], s[sreg_wval+0*10+9+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+6], s[sreg_wval+1*10+0+48]
    v_mac_f32 v[vreg_oval+5], v[vreg_ival+7], s[sreg_wval+1*10+1+48]
    .if use_ds_read_b128
    macro_ds_read_b128 vreg_ival+4, vreg_inp_dsrd0, 4*(3*136+6)
    s_waitcnt lgkmcnt(1)
    .else
    ds_read_b64 v[vreg_ival+4:vreg_ival+5], v[vreg_inp_dsrd0] offset:4*(3*136+6)
    ds_read_b64 v[vreg_ival+6:vreg_ival+7], v[vreg_inp_dsrd0] offset:4*(3*136+8)
    s_waitcnt lgkmcnt(2)
    .endif
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+0], s[sreg_wval+3*10+2]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+1], s[sreg_wval+3*10+3]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+2], s[sreg_wval+3*10+4]
    v_mac_f32 v[vreg_oval+0], v[vreg_ival+3], s[sreg_wval+3*10+5]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+0], s[sreg_wval+1*10+2]
    v_mac_f32 v[vreg_oval+1], v[vreg_ival+1], s[sreg_wval+1*10+3]
    v