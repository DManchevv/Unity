to the middle of the int range) */
.endm

.macro .GPR_REUSE __gpr_number_symbolic_old, __gpr_number_symbolic_new
    .set \__gpr_number_symbolic_new, \__gpr_number_symbolic_old
    .GPR_INVALIDATE \__gpr_number_symbolic_old
.endm


// initial state (s[0:4] are overlapped with filtersA):
// s[0:1] - kernarg address
// s2 - wg x (none)
// s3 - wg y (C)
// s4 - wg z (K)
kernarg = 0
gid_x = 2
gid_y = 3
gid_z = 4

// kernarg layout:
// dwords 0:4 - n, c, H, W, k
// dwords 5:7 - not used
// dwords 8:9 - input buffer pointer
// dwords 10:11 - weights pointer
// dwords 12:13 - output buffer pointer
// dwords 14:15 - debug buffer pointer
.set in_ptr_off, 0x20
.set wei_ptr_off, 0x28
.set out_ptr_off, 0x30
.set dbg_ptr_off, 0x38

/*******************************************************************************
 *
 * MIT License
 *
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 *******************************************************************************/

.ifdef EXPERIMENTAL_COv3
    .ifdef .option.machine_version_major
        .error ".option.machine_version_major already defined"
        .end
    .endif
    .set .option.machine_version_major, .amdgcn.gfx_generation_number
.endif

LAYOUT_DATA_NCHW = 0
LAYOUT_DATA_CNHW = 1
LAYOUT_DATA_NHWC = 2
LAYOUT_DATA_CHWN = 3

// acc_type and buf_type: 0 - FP64, 1 - FP32, 2 - FP16, 5 - int32, 6 - int16, 7 - int8
TYPE_FP64  = 0
TYPE_FP32  = 1
TYPE_FP16  = 2
TYPE_BFP16 = 3
TYPE_INT64 = 4
TYPE_INT32 = 5
TYPE_INT16 = 6
TYPE_INT8  = 7
TYPE_INT4  = 8

.macro log2 lg2, num, max_bits=8
    \lg2 = 0
    lg_i = \num
    .rept \max_bits
        lg_i = lg_i / 2
        .if lg_i > 0
            \lg2 = \lg2 + 1
        .endif
    .endr
.endm

.macro default symbol, value
    .ifnotdef \symbol
        \symbol = \value
    .endif
.endm

.macro static_assert fufufu
    .if !\fufufu
        .error "\fufufu is false"
        .end
    .endif
.endm

.macro swap a, b
    __tmp = \a
    \a = \b
    \b = __tmp
.endm

.macro m_bpermute vgpr, cnt, addr
    v = \vgpr
    .rept \cnt
        ds_bpermute_b32 v[v], v[\addr], v[v]
        v = v + 1
    .endr
.endm

.macro m_swizzle vgpr, cnt, pattern
    v = \vgpr
    .rept \cnt
        ds_swizzle_b32 v[v], v[v] offset:\pattern
        v = v + 1
    .endr
.endm

.if (.option.machine_version_major == 8)
    .set max_hw_vctn, 15
.elseif (.option.machine_version_major == 9)
    .set max_hw_vctn, 63
.endif
max_hw_lcnt = 15
.macro s_wait vmcnt=max_hw_vctn, lgkmcnt=max_hw_lcnt
    vm_cnt = \vmcnt
    lgkm_cnt = \lgkmcnt
    .if vm_cnt > max_hw_vctn
        vm_cnt = max_hw_vctn
    .elseif vm_cnt < 0
        vm_cnt = 0
    .endif
    .if lgkm_cnt > max_hw_lcnt
        lgkm_cnt = max_hw_lcnt
    .elseif lgkm_cnt < 0
        lgkm_cnt = 0
    .endif
    s_waitcnt vmcnt(0 + vm_cnt) & lgkmcnt(0 + lgkm_cnt)
.endm


maxU24 = 1 << 24

wave_size = 64
log2 wave_size_log2, wave_size

.macro m_buffer_load_dwordx size, dst, off, desc, soff, ioff=0
    .if \size == 1
        buffer_load_dword v[\dst], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .elseif \size == 2
        buffer_load_dwordx2 v[\dst:\dst+\size-1], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .elseif \size == 3
        buffer_load_dwordx3 v[\dst:\dst+\size-1], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .elseif \size == 4
        buffer_load_dwordx4 v[\dst:\dst+\size-1], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .endif
.endm

.macro m_buffer_load_ushort size, dst, off, desc, soff, ioff=0
    .if \size == 1
        buffer_load_ushort v[\dst], v[\off],  s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .endif
.endm

.macro m_buffer_store_short size, src, off, desc, soff, ioff=0
    .if \size == 1
        buffer_store_short v[\src], v[\off],  s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .endif
.endm

.macro u32_div numer, denom, uquo, v_tmp, s_tmp
    u_div   v[\numer], v[\denom] v[\uquo], \v_tmp, \s_tmp, \s_tmp + 2 
.endm


// Unsigned division function from the SC implementation
// 4 s_tmps, 4 v_tmps
//
.macro u_div numer, denom, uquo, vtmp, stmp1, stmp2
    v_cvt_f32_u32     v[\vtmp],     \denom
    v_rcp_f32         v[\vtmp],     v[\vtmp]
    v_mul_f32         v[\vtmp],     0x4f800000,   v[\vtmp]
    v_cvt_u32_f32     v[\vtmp],     v[\vtmp]

    v_mul_lo_u32      v[\vtmp+1],   \denom,       v[\vtmp]
    v_mul_hi_u32      v[\vtmp+2],   \denom,       v[\vtmp]
   _v_sub_co_u32      v[\vtmp+3],   vcc,          0,           v[\vtmp+1]
    v_cmp_ne_i32      s[\stmp1:\stmp1+1], 0,      v[\vtmp+2]
    v_cndmask_b32     v[\vtmp+1],   v[\vtmp+3],   v[\vtmp+1],  s[\stmp1:\stmp1+1]
    v_mul_hi_u32      v[\vtmp+1],   v[\vtmp+1],   v[\vtmp]
   _v_sub_co_u32      v[\vtmp+2],   vcc,          v[\vtmp],    v[\vtmp+1]
    v_add_co_u32      v[\vtmp],     vcc,          v[\vtmp],    v[\vtmp+1]
    v_cndmask_b32     v[\vtmp],     v[\vtmp],     v[\vtmp+2],  s[\stmp1:\stmp1+1]
    v_mul_hi_u32      v[\vtmp],     v[\vtmp],     \numer
    v_mul_lo_u32      v[\vtmp+1],   v[\vtmp],     \denom
   _v_sub_co_u32      v[\vtmp+2],   vcc,          \numer,      v[\vtmp+1]
    v_cmp_ge_u32      s[\stmp1:\stmp1+1],         \numer,      v[\vtmp+1]
    v_cmp_ge_u32      s[\stmp2:\stmp2+1],         v[\vtmp+2],  \denom
   _v_add_co_u32      v[\vtmp+2],   vcc,          1,           v[\vtmp]
    s_and_b64         s[\stmp2:\stmp2+1], s[\stmp1:\stmp1+1],  s[\stmp2:\stmp2+1]
   _v_add_co_u32      v[\vtmp+1],   vcc, -1,      v[\vtmp]
    v_cndmask_b32     v[\vtmp+2],   v[\vtmp],     v[\vtmp+2],  s[\stmp2:\stmp2+1]
    v_cndmask_b32     v[\vtmp+2],   v[\vtmp+1],   v[\vtmp+2],  s[\stmp1:\stmp1+1]
    v_cmp_ne_i32      vcc,          0,            \denom
    v_cndmask_b32     \uquo,        -1,           v[\vtmp+2],  vcc
.endm

.altmacro
.macro ceil_2_32_div_u16 m, denom, vtmp, stmp
	v_cvt_f32_u32     v[\vtmp],     \denom
	v_rcp_f32         v[\vtmp],     v[\vtmp]
	v_mul_f32         v[\vtmp],     0x4f800000,   v[\vtmp]
	v_cvt_u32_f32     v[\vtmp],     v[\vtmp]

	v_mul_lo_u32      v[\vtmp+1],   \denom,       v[\vtmp]
	v_mul_hi_u32      v[\vtmp+2],   \denom,       v[\vtmp]
	v_sub_u32         v[\vtmp+3],   0,            v[\vtmp+1]
	v_cmp_ne_i32      s[\stmp:\stmp+1], 0,        v[\vtmp+2]
	v_cndmask_b32     v[\vtmp+1],   v[\vtmp+3],   v[\vtmp+1],  s[\stmp:\stmp+1]
	v_mul_hi_u32      v[\vtmp+1],   v[\vtmp+1],   v[\vtmp]
	v_sub_u32         v[\vtmp+2],   v[\vtmp],     v[\vtmp+1]
	v_add_co_u32      v[\vtmp],     vcc,          v[\vtmp],    v[\vtmp+1]
	v_cndmask_b32     v[\vtmp],     v[\vtmp],     v[\vtmp+2],  s[\stmp:\stmp+1]
	v_mul_hi_u32      v[\vtmp],     -1,           v[\vtmp]
	v_mul_lo_u32      v[\vtmp+1],   v[\vtmp],     \denom
	v_sub_u32         v[\vtmp+2],   -1,           v[\vtmp+1]
	v_cmp_ge_u32      s[\stmp:\stmp+1],           v[\vtmp+2],  \denom
	v_add_u32         v[\vtmp+2],   1,            v[\vtmp]
	v_add_co_u32      v[\vtmp+1],   vcc, -1,      v[\vtmp]
	v_cndmask_b32     v[\vtmp+2],   v[\vtmp],     v[\vtmp+2],  s[\stmp:\stmp+1]
	v_add_u32         v[\vtmp+2],   1,            v[\vtmp+2]
	v_cmp_ne_i32      vcc,          0,            \denom
	v_cndmask_b32     \m,        -1,           v[\vtmp+2],  vcc
.endm

.macro disable_srd srd
	s_mov_b32 s[\srd+3], 0
.endm
.macro enable_srd srd
	s_mov_b32 s[\srd+3], 0x00020000            // DATA_FORMAT, need to just be non-zero;
.endm

.macro label l, n
	\l\n:
.endm
.macro _s_cbranch cond, l, n
	s_cbranch_\cond \l\n
.endm
.macro _s_branch l, n
	s_branch \l\n
.endm


div_const_1_2=0x80000000
div_const_1_3=0x55555556
div_const_1_4=0x40000000
div_const_1_5=0x33333334
div_const_1_6=0x2aaaaaab
div_const_1_7=0x24924925
div_const_1_8=0x20000000
div_const_1_9=0x1c71c71d
div_const_1_10=0x1999999a
div_const_1_11=0x1745d175
div_const_1_12=0x15555556
div_const_1_13=0x13b13b14
div_const_1_14=0x12492493
div_const_1_15=0x11111112
div_const_1_16=0x10000000
div_const_1_17=0x0f0f0f10
div_const_1_18=0x0e38e38f
div_const_1_19=0x0d79435f
div_const_1_20=0x0ccccccd
div_const_1_21=0x0c30c30d
div_const_1_22=0x0ba2e8bb
div_const_1_23=0x0b21642d
div_const_1_24=0x0aaaaaab
div_const_1_25=0x0a3d70a4
div_const_1_26=0x09d89d8a
div_const_1_27=0x097b425f
div_const_1_28=0x0924924a
div_const_1_29=0x08d3dcb1
div_const_1_30=0x08888889
div_const_1_31=0x08421085
div_const_1_32=0x08000000
div_const_1_33=0x07c1f07d
div_const_1_34=0x07878788
div_const_1_35=0x07507508
div_const_1_36=0x071c71c8
div_const_1_37=0x06eb3e46
div_const_1_38=0x06bca1b0
div_const_1_39=0x06906907
div_const_1_40=0x06666667
div_const_1_41=0x063e7064
div_const_1_42=0x06186187
div_const_1_43=0x05f417d1
div_const_1_44=0x05d1745e
div_const_1_45=0x05b05b06
div_const_1_46=0x0590b217
div_const_1_47=0x0572620b
div_const_1_48=0x05555556
div_const_1_49=0x0539782a
div_const_1_50=0x051eb852
div_const_1_51=0x05050506
div_const_1_52=0x04ec4ec5
div_const_1_53=0x04d4873f
div_const_1_54=0x04bda130
div_const_1_55=0x04a7904b
div_const_1_56=0x04924925
div_const_1_57=0x047dc120
div_const_1_58=0x0469ee59
div_const_1_59=0x0456c798
div_const_1_60=0x04444445
div_const_1_61=0x04325c54
div_const_1_62=0x04210843
div_const_1_63=0x04104105
div_const_1_64=0x04000000

.macro _s_div_const_u32_u16 dst, src, denum
	.if \denum == 1
		s_mov_b32 \dst, \src
	.elseif \denum >=2 && \denum <= 64
		s_mul_hi_u32 \dst, div_const_1_\denum, \src
	.else
		static_assert(0)
	.endif
.endm

.macro _v_div_const_u32_u16 dst, src, denum, tmp
	.if \denum == 1
		v_mov_b32 \dst, \src
	.elseif \denum >=2 && \denum <= 64
		s_mov_b32 \tmp, div_const_1_\denum
		v_mul_hi_u32 \dst, \tmp, \src
	.else
		static_assert(0)
	.endif
.endm

.macro _s_ceil_u32 dst, src, denum
	s_add_u32 \dst, \denum - 1, \src
	_s_div_const_u32_u16 \dst, \dst, \denum
.endm


default c_per_wave, 4
default k_per_wave, 4
default n_per_group, 1
default pipe_lines_depth, 2
default chunk_size, 16
default reverse_inout, 0
default weights_layout, 0
default reverse_weights, 0

default elements_in_dword, 1
static_assert(elements_in_dword == 1 || elements_in_dword == 2)
.if elements_in_dword == 2
   static_assert ((batch_size % elements_in_dword) == 0)
   static_assert (.option.machine_version_major >= 9)
.endif
vec_size = elements_in_dword

.if reverse_inout
   static_assert (stride_h == 1 && stride_w == 1)
   swap input_channels, output_channels
   swap in_ptr_off, out_ptr_off
   swap gid_y, gid_z
   reverse_weights = !reverse_weights
   weights_layout = !weights_layout
.endif

static_assert (pad_h == 1 && pad_w == 1)
static_assert (stride_h == 1 || stride_h == 2)
static_assert (stride_w == 1 || stride_w == 2)
.if reverse_inout
   static_assert (stride_h == 1 && stride_w == 1)
.endif
static_assert (wei_h == 3 && wei_w == 3)
static_assert (img_w <= 512)
static_assert (pipe_lines_depth <= img_h)
static_assert (pad_h < wei_h)
static_assert (input_channels % c_per_wave == 0)
static_assert (output_channels % k_per_wave == 0)
static_assert (1 <= n_per_group && n_per_group <= 8)
static_assert (n_per_group <= batch_size)
static_assert (c_per_wave * chunk_size == 64)
static_assert (k_per_wave * chunk_size <= 64)

log2 c_per_wave_log2, c_per_wave
log2 k_per_wave_log2, k_per_wave

/*******************************************************************************
 * 
 * MIT License
 * 
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 * 
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 * 
 *******************************************************************************/

nbytes = 4 / vec_size
// weights
weights_per_filter = wei_w * wei_h
.if weights_layout == 0 // KCHW
	.set filter_c_stride, nbytes * weights_per_filter
	.set filter_k_stride, nbytes * weights_per_filter * input_channels / group_counts
.else // CKHW
	.set filter_c_stride, nbytes * weights_per_filter * output_channels / group_counts
	.set filter_k_stride, nbytes * weights_per_filter
.endif
filters_size = nbytes * weights_per_filter * input_channels * output_channels / group_counts

// input/output
img_hw = img_h * img_w
img_hw_vec = (img_hw + vec_size - 1) / vec_size
odd_hw = (img_h * img_w) % vec_size
out_w = (img_w + 2 * pad_w - wei_w) / stride_w + 1;
out_h = (img_h + 2 * pad_h - wei_h) / stride_h + 1;
input_line_size = nbytes * img_w

input_c_stride = input_line_size * img_h
input_feature_map_size = input_c_stride

input_n_stride = input_feature_map_size * input_channels
input_stack_size = input_n_stride

output_line_size = nbytes * out_w

output_k_stride = output_line_size * out_h
output_feature_map_size = output_k_stride

output_n_stride = output_feature_map_size * output_channels
output_stack_size = output_n_stride


maxU24 = 1 << 24
static_assert (filter_c_stride < maxU24)
static_assert (filter_k_stride < maxU24)
static_assert (input_feature_map_size < maxU24)
static_assert (output_feature_map_size < maxU24)

w_half_in = img_w % elements_in_dword
w_half_out = out_w % elements_in_dword
img_w_vec = (img_w + elements_in_dword - 1) / elements_in_dword
out_w_vec = (out_w + elements_in_dword - 1) / elements_in_dword

// chunk parameters
log2 chunk_size_log2, chunk_size
chunks_in = (img_w_vec + chunk_size - 1) / chunk_size
.if (chunk_size != 16)
   // force chunks to have enough zeros for padding
   chunks_in = (img_w_vec + chunk_size - pad_w - 1) / (chunk_size - pad_w)
.endif
.if (chunks_in % stride_w) && (chunks_in > 1)
   // force chunks to be aligned with stride
   chunks_in = chunks_in + chunks_in % stride_w
.endif
.if chunks_in > 1
   chunks_out = chunks_in / stride_w
   static_assert ((chunks_out * stride_w) == chunks_in)
.else
   chunks_out = 1
.endif
active_in_lanes = (img_w_vec + chunks_in - 1) / chunks_in // active lanes in chunk
active_out_lanes = (out_w_vec + chunks_out - 1) / chunks_out
static_assert (active_in_lanes == active_out_lanes || chunks_in == 1)
active_lanes = active_in_lanes
full_chunks_in = img_w_vec % chunks_in
full_chunks_out = out_w_vec % chunks_out
.if full_chunks_in == 0
   full_chunks_in = chunks_in
.endif
.if full_chunks_out == 0
   full_chunks_out = chunks_out
.endif
partial_chunks_in = chunks_in - full_chunks_in
partial_chunks_out = chunks_out - full_chunks_out
mbufs_per_line_in = (full_chunks_in + 3) / 4 + (partial_chunks_in + 3) / 4 // memory buffer instructions per line
mbufs_per_line_out = (full_chunks_out + 3) / 4 + (partial_chunks_out + 3) / 4 // memory buffer instructions per line
mbufs_per_line_in = mbufs_per_line_in + w_half_in
mbufs_per_line_out = mbufs_per_line_out + w_half_out
gprs_per_line_in = full_chunks_in + partial_chunks_in
gprs_per_line_out = full_chunks_out + partial_chunks_out
gprs_per_batch_in = gprs_per_line_in * lines_cnt_in
gprs_per_batch_out = gprs_per_line_out * lines_cnt_out


static_assert ((chunk_size == 16) || (chunk_size == 64) || (active_lanes < chunk_size)) // 64 for future expansion
static_assert (chunk_size == 8 || chunk_size == 16)
active_lanes_mask = (1 << active_lanes) - 1
partial_lanes_mask = 1 << (active_lanes - 1)
shift = chunk_size
.rept 5 - chunk_size_log2
   active_lanes_mask = active_lanes_mask + (active_lanes_mask << shift)
   partial_lanes_mask = partial_lanes_mask + (partial_lanes_mask << shift)
   shift = shift * 2
.endr

static_assert(active_lanes_mask <= 0xffffffff)
static_assert(partial_lanes_mask <= 0xffffffff)

input_buffer_size = input_stack_size * batch_size
output_buffer_size = output_stack_size * batch_size

.if (.option.machine_version_major == 8)
   .set max_hw_vctn, 15
.elseif (.option.machine_version_major == 9)
   .set max_hw_vctn, 63
.endif
max_hw_lcnt = 15

.GPR_ALLOC_BEGIN
.if limit_wave_cnt
   .SET_MAX_WAVES_LIMIT limit_wave_cnt
.endif

.SGPR_ALLOC_FROM 5
.SGPR_ALLOC soffset_in
.SGPR_ALLOC soffset_out
.SGPR_ALLOC soffset_wei
.SGPR_ALLOC desc_in, 4 // input buffer descriptor
.SGPR_ALLOC desc_out, 4   // weights buffer descriptor
.SGPR_ALLOC desc_wei, 4   // output buffer descriptor
.if k_group_size_is_power_of_two
    .SGPR_ALLOC stmp
.else
    .SGPR_ALLOC stmp, 4
.endif
.SGPR_ALLOC loop_n_cnt
.SGPR_ALLOC loop_h_cnt
.SGPR_ALLOC wave_id // wave_id in group
.SGPR_RESERVE_XNACK



.VGPR_ALLOC_FROM 0
.VGPR_ALLOC tid
.VGPR_ALLOC voffset_in
.VGPR_ALLOC voffset_out
.VGPR_ALLOC voffset_part_in
.VGPR_ALLOC voffset_part_out
accums_cnt = wei_w * wei_h * c_per_wave * k_per_wave * chunk_size / 64
lines_cnt_in = pipe_lines_depth + wei_h - 1
lines_cnt_out = (pipe_lines_depth + stride_h - 1) / stride_h
.VGPR_ALLOC accums, accums_cnt
.VGPR_ALLOC lines_in, gprs_per_line_in * lines_cnt_in * elements_in_dword
.VGPR_ALLOC lines_out, gprs_per_line_out * lines_cnt_out * elements_in_dword
.VGPR_ALLOC permute_addr
.if elements_in_dword == 2
   .VGPR_ALLOC shfl
.endif

.if k_group_size_is_power_of_two || gprs_per_line_in * lines_cnt_in * elements_in_dword >= 4
    vtmp_udiv = lines_in
.else
    .VGPR_ALLOC vtmp_udiv, 4
.endif

.if k_group_size_is_power_of_two || gprs_per_line_out * lines_cnt_out * elements_in_dword >= 3
    gid = lines_out
    group_id = lines_out + 1
    group_size = lines_out + 2
.else
    .VGPR_ALLOC gid
    .VGPR_ALLOC group_id
    .VGPR_ALLOC group_size
.endif

.LDS_ALLOC_FROM 0
.LDS_ALLOC accums_lds, (n_per_group - 1) * 64 * 4 * accums_cnt

.GPR_ALLOC_END

max_waves_per_CU = (256 / .AUTO_VGPR_COUNT) * 4
static_assert( max_waves_per_CU >= n_per_group )
//.text 0
//.p2align 8
gcnAsmConv3x3WrW:

   .amd_kernel_code_t
    enable_sgpr_kernarg_segment_ptr = 1
    enable_sgpr_workgroup_id_x = 1
    enable_sgpr_workgroup_id_y = 1
    enable_sgpr_workgroup_id_z = 1
    is_ptr64 = 1
    granulated_workitem_vgpr_count = .AUTO_VGPR_GRANULATED_COUNT
    granulated_wavefront_sgpr_count = .AUTO_SGPR_GRANULATED_COUNT
    enable_vgpr_workitem_id = 1
    user_sgpr_count = 2
    kernarg_segment_byte_size = 64
    wavefront_sgpr_count = .AUTO_SGPR_COUNT
    workitem_vgpr_count = .AUTO_VGPR_COUNT
    float_mode = 192
    workgroup_group_segment_byte_size = .AUTO_LDS_BYTE_SIZE
   .end_amd_kernel_code_t


   s_load_dwordx2 s[desc_in:desc_in+1], s[kernarg:kernarg+1], 0x0 + in_ptr_off
   s_load_dwordx2 s[desc_wei:desc_wei+1], s[kernarg:kernarg+1], 0x0 + wei_ptr_off
   s_load_dwordx2 s[desc_out:desc_out+1], s[kernarg:kernarg+1], 0x0 + out_ptr_off


   // fill format and size fields of buffer descriptors
   static_assert ((.option.machine_version_major == 8) || (.option.machine_version_major == 9))
   s_mov_b32 s[desc_in+2], input_buffer_size
   s_mov_b32 s[desc_in+3], 0x00027000
   s_mov_b32 s[desc_wei+2], filters_size
   s_mov_b32 s[desc_wei+3], 0x00027000
   s_mov_b32 s[desc_out+2], output_buffer_size
   s_mov_b32 s[desc_out+3], 0x00027000

   vtmp = accums
   vtmp2 = voffset_part_in
   v_lshrrev_b32 v[vtmp], 6, v[tid]
   v_readfirstlane_b32 s[wave_id], v[vtmp]
   v_and_b32 v[tid], 0x3f, v[tid]

   // calculate input/output offsets
   // example for c_per_wave=4, k_per_wave=2
   // lanes  0-15: c0, k0
   // lanes 16-31: c1, k1
   // lanes 32-47: c2, k0
   // lanes 48-63: c3, k1

   v_lshrrev_b32 v[vtmp], 0 + chunk_size_log2, v[tid] // vtmp = wave part id
   v_mul_u32_u24 v[voffset_in], 0 + input_feature_map_size, v[vtmp]
   v_and_b32 v[voffset_out], 0 + (1 << k_per_wave_log2) - 1, v[vtmp]
   v_mul_u32_u24 v[voffset_out], 0 + output_feature_map_size, v[voffset_out]

   i = 0
   .rept accums_cnt
      v_mov_b32 v[accums+i], 0
      i = i + 1
   .endr

   v_and_b32 v[vtmp2], 0 + (1 << chunk_size_log2) - 1, v[tid] // vtmp = lane in wave part
   v_mul_u32_u24 v[vtmp], 4 * gprs_per_line_in, v[vtmp2]
  _v_add_co_u32 v[voffset_in], vcc, v[voffset_in], v[vtmp]
   .if stride_w == 1 || chunks_in > 1
      v_mul_u32_u24 v[vtmp], 4 * gprs_per_line_out, v[vtmp2]
     _v_add_co_u32 v[voffset_out], vcc, v[voffset_out], v[vtmp]
   .else
      static_assert (stride_w == 2)
      v_lshrrev_b32 v[vtmp2], 1, v[vtmp2]
      v_mul_u32_u24 v[vtmp], 4 * gprs_per_line_out, v[vtmp2]
     _v_add_co_u32 v[voffset_out], vcc, v[voffset_out], v[vtmp]
      s_mov_b32 exec_lo, 0xAAAAAAAA
      s_mov_b32 exec_hi, 0xAAAAAAAA
      v_mov_b32 v[voffset_out], 0x80000000
      s_mov_b32 exec_lo, -1
      s_mov_b32 exec_hi, -1
   .endif

   .GPR_INVALIDATE vtmp
   .GPR_INVALIDATE vtmp2

   // calculate offsets for partial chunks
   v_mov_b32 v[voffset_part_in], v[voffset_in]
   v_mov_b32 v[voffset_part_out], v[voffset_out]
   s_mov_b32 exec_lo, partial_lanes_mask
   s_mov_b32 exec_hi, partial_lanes_mask
   v_mov_b32 v[voffset_part_in], 0x80000000
   v_mov_b32 v[voffset_part_out], v[voffset_part_in]

   s_mov_b32 exec_lo, active_lanes_mask
   s_mov_b32 exec_hi, active_lanes_mask

   s_waitcnt 0

   // calculate buffer offsets
   s_mul_i32 s[stmp], s[wave_id], input_stack_size // image in batch (n)
   s_mul_i32 s[soffset_in], s[gid_y], c_per_wave * input_feature_map_size // input feature map (c)
   s_add_u32 s[soffset_in], s[soffset_in], s[stmp]
   s_mul_i32 s[stmp], s[wave_id], output_stack_size // image in batch (n)
   s_mul_i32 s[soffset_out], s[gid_z], k_per_wave * output_feature_map_size // output feature map (k)
   s_add_u32 s[soffset_out], s[soffset_out], s[stmp]
   s_mul_i32 s[soffset_wei], s[gid_y], c_per_wave * filter_c_stride
   s_mul_i32 s[stmp], s[gid_z], k_per_wave * filter_k_stride
   s_add_u32 s[soffset_wei], s[soffset_wei], s[stmp]

   // calculate group offsets
   static_assert(output_channels % (k_per_wave * group_counts) == 0)
   static_assert(input_channels % (c_per_wave * group_counts) == 0)
   .if reverse_inout
       c_group_size = output_channels / k_per_wave / group_counts
       k_group_size = input_channels / c_per_wave / group_counts
       .if k_group_size_is_power_of_two
           log2 k_group_size_log2, k_group_size
           s_lshr_b32 s[stmp], s[gid_y], 0 + k_group_size_log2 // group_id
       .else
           v_mov_b32 v[gid], s[gid_y]
           v_mov_b32 v[group_size], 0 + k_group_size
           u32_div gid, group_size, group_id, vtmp_udiv, stmp
           v_readfirstlane_b32 s[stmp], v[group_id]
       .endif
       s_mul_i32 s[stmp], s[stmp], c_group_size * k_per_wave * output_feature_map_size // k_group_offset
       s_add_u32 s[soffset_out], s[soffset_out], s[stmp]
   .else
       k_group_size = output_channels / k_per_wave / group_counts
       c_group_size = input_channels / c_per_wave / group_counts
       .if k_group_size_is_power_of_two
           log2 k_group_size_log2, k_group_size
           s_lshr_b32 s[stmp], s[gid_z], 0 + k_group_size_log2 // group_id
       .else
           v_mov_b32 v[gid], s[gid_z]
           v_mov_b32 v[group_size], 0 + k_group_size
           u32_div gid, group_size, group_id, vtmp_udiv, stmp
           v_readfirstlane_b32 s[stmp], v[group_id]
       .endif
       s_mul_i32 s[stmp], s[stmp], c_group_size * c_per_wave * input_feature_map_size // c_group_offset
       s_add_u32 s[soffset_in], s[soffset_in], s[stmp]
   .endif

   .GPR_INVALIDATE gid
   .GPR_INVALIDATE group_size
   .GPR_INVALIDATE group_id
   .GPR_INVALIDATE vtmp_udiv

   s_mov_b32 s[loop_n_cnt], 0

   .macro .single_vload base, v_offset, s_offset, desc, mbufs_inflight, count
      .if ((vals_to_load - \count) >= 0) && vals_to_load > 0
         .if imm_off >= (1 << 12)
            .error "Error: Immediate offset is too large for buffer_load instruction"
         .endif

         .if \count == 1
            buffer_load_dword v[\base+vals_loaded], v[\v_offset], s[\desc:\desc+3], s[\s_offset] offen offset:0+imm_off
         .else
            buffer_load_dwordx\count v[\base+vals_loaded:\base+vals_loaded+\count-1], v[\v_offset], s[\desc:\desc+3], s[\s_offset] offen offset:0+imm_off
         .endif

         \mbufs_inflight = \mbufs_inflight + 1
         vals_to_load = vals_to_load - \count
         vals_loaded = vals_loaded + \count
         imm_off = imm_off + 4 * \count
      .endif
   .endm

   .macro .next_line inout, line
      .if \line >= lines_cnt_\inout - 1
         \line = 0
      .else
         \line = \line + 1
      .endif
   .endm

   .macro exch_vgpr, img_c0, img_c1
      v_mov_b32 v[shfl], v[\img_c0]
      v_mov_b32_sdwa v[\img_c0], v[\img_c1] dst_sel:WORD_1 src0_sel:WORD_0
      v_mov_b32_sdwa v[\img_c1], v[shfl] dst_sel:WORD_0 src0_sel:WORD_1
   .endm

   .macro exch_line_f16 inout
      .if elements_in_dword == 2
         v_cnt = 0
         line_base = lines_\inout + gprs_per_line_\inout * exch_line_\inout
         .rept gprs_per_line_\inout
            exch_vgpr line_base + v_cnt, line_base + v_cnt + gprs_per_batch_\inout
            v_cnt = v_cnt + 1
         .endr
         exch_line_\inout = exch_line_\inout + 1
         .if(exch_line_\inout == lines_cnt_\inout)
            exch_line_\inout = 0
         .endif
      .endif
   .endm

   .macro exch_step_f16
      .if g_line_exch < img_h
          .if g_line_exch < img_h - 1
             exch_line_f16 in
          .endif
          .if !(g_line_exch % stride_h)
             exch_line_f16 out
          .endif
      .endif
      g_line_exch = g_line_exch + 1
   .endm

   .macro .load_line inout, line, mbufs_inflight, go_to_next_line = 1
      so = soffset_\inout
      //reserve soffset
      s_mov_b32 s[stmp], s[so]
      n_cnt = 0
      .rept elements_in_dword
          vo = voffset_\inout
          desc = desc_\inout
          vals_to_load = full_chunks_\inout
          vals_loaded = 0
          imm_off = 0
          line_base = lines_\inout + gprs_per_line_\inout * \line + n_cnt * gprs_per_batch_\inout

          .rept (full_chunks_\inout / 4)
             .single_vload line_base, vo, so, desc, \mbufs_inflight, 4
          .endr
          .single_vload line_base, vo, so, desc, \mbufs_inflight, 3
          .single_vload line_base, vo, so, desc, \mbufs_inflight, 2
          .single_vload line_base, vo, so, desc, \mbufs_inflight, 1

          vals_to_load = partial_chunks_\inout
          vo = voffset_part_\inout
          .rept (partial_chunks_\inout / 4)
             .single_vload line_base, vo, so, desc, \mbufs_inflight, 4
          .endr
          .single_vload line_base, vo, so, desc, \mbufs_inflight, 3
          .single_vload line_base, vo, so, desc, \mbufs_inflight, 2
          .single_vload line_base, vo, so, desc, \mbufs_inflight, 1

          .if w_half_\inout
             s_mov_b32 exec_lo, partial_lanes_mask
             s_mov_b32 exec_hi, partial_lanes_mask
             buffer_load_ushort v[line_base + vals_loaded - partial_chunks_\inout - 1], v[voffset_\inout], s[desc:desc+3], s[so] offen offset:0 + (vals_loaded - partial_chunks_\inout - 1) * 4
             s_mov_b32 exec_lo, active_lanes_mask
             s_mov_b32 exec_hi, active_lanes_mask
          .endif

         .if so == soffset_in
            s_add_u32 s[so], s[so], 0 + input_stack_size * n_per_group
         .else
            s_add_u32 s[so], s[so], 0 + output_stack_size * n_per_group
         .endif
         n_cnt = n_cnt + 1
      .endr

      //restore soffset
      s_mov_b32 s[so], s[stmp]

      .if \go_to_next_line
         .next_line \inout, \line
         .if so == soffset_in
            s_add_u32 s[so], s[so], input_line_size
         .else
            s_add_u32 s[so], s[so], output_line_size
         .endif
      .endif
   .endm

   .if stride_w == 2
      line_adjust = gprs_per_batch_in
      line_adjust_1 = 1
   .else
      line_adjust = 0
      line_adjust_1 = gprs_per_batch_in
   .endif

   .macro conv_line in_line, out_line, acc_line, acc_batch, sync=0, swizzle=0
      in_base = lines_in + gprs_per_line_in * \in_line
      out_base = lines_out + gprs_per_line_out * \out_line
      acc_base = accums + wei_w * \acc_line + weights_per_filter * \acc_batch
      out_x = 0 // current gpr in line
      .rept gprs_per_line_out
         acc_x = 0
         .if \sync
            s_wait , gprs_per_line_out-out_x-1
         .endif
         .rept wei_w
            in_x = out_x * stride_w - pad_w + acc_x
            .if reverse_weights
               acc_off = wei_w - acc_x - 1
            .else
               acc_off = acc_x
            .endif
            .if in_x < 0
               .if elements_in_dword == 2
                  v_mov_b32 v[shfl], v[in_base + gprs_per_line_in - 1 + gprs_per_batch_in] row_shr:1 bound_ctrl:0
                  v_dot2  acc_base + acc_off, shfl, out_base + out_x
                  v_dot2  acc_base + acc_off, in_base + line_adjust, out_base + out_x + gprs_per_batch_out
               .else
                  v_mac_f32 v[acc_base + acc_off], v[in_base+gprs_per_line_in-1], v[out_base + out_x] row_shr:1 bound_ctrl:0
               .endif
            .elseif in_x >= gprs_per_line_in
                  .if elements_in_dword == 2
                     v_dot2  acc_base + acc_off, in_base + gprs_per_line_in - 1 + gprs_per_batch_in, out_base + out_x
                     .if gprs_per_line_in == 1
                         v_mov_b32 v[shfl], v[in_base + line_adjust] row_shl:1 bound_ctrl:0
                     .else
                         v_mov_b32 v[shfl], v[in_base] row_shl:1 bound_ctrl:0
                     .endif
                     v_dot2  acc_base + acc_off, shfl, out_base + out_x + gprs_per_batch_out
               .else
                  v_mac_f32 v[acc_base + acc_off], v[in_base], v[out_base + out_x] row_shl:1 bound_ctrl:0
               .endif
            .else
               .if elements_in_dword == 2
                  .if acc_x == 1
                     v_dot2  acc_base + acc_off, in_base + in_x, out_base + out_x
                     .if stride_w == 2 && gprs_per_line_in == 1
                        v_mov_b32 v[shfl], v[in_base] row_shl:1 bound_ctrl:0
                        v_dot2  acc_base + acc_off, shfl, out_base + out_x + gprs_per_batch_out
                     .else
                        v_dot2 acc_base + acc_off, in_base + in_x + line_adjust_1, out_base + out_x + gprs_per_batch_out
                     .endif
                  .elseif acc_x == 0
                     v_dot2  acc_base + acc_off, in_base + in_x + gprs_per_batch_in, out_base + out_x
                     v_dot2  acc_base + acc_off, in_base + in_x + 1 + line_adjust, out_base + out_x + gprs_per_batch_out
                  .elseif acc_x == 2
                     v_dot2  acc_base + acc_off, in_base + in_x - 1 + gprs_per_batch_in, out_base + out_x
                     v_dot2  acc_base + acc_off, in_base + in_x + line_adjust, out_base + out_x + gprs_per_batch_out
                  .else
                     static_assert(0)
                  .endif
               .else
                  v_mac_f32 v[acc_base + acc_off], v[in_base + in_x], v[out_base + out_x]
               .endif
            .endif
            acc_x = acc_x + 1
         .endr
         .if \swizzle==32 // swaps each 32 lanes
            // lanes[ 0:31] <-> lanes[32:63]
            ds_bpermute_b32 v[out_base+out_x], v[permute_addr], v[out_base+out_x]
            .if elements_in_dword == 2
               ds_bpermute_b32 v[out_base+out_x+gprs_per_batch_out], v[permute_addr], v[out_base+out_x+gprs_per_batch_out]
            .endif
         .elseif \swizzle==16  // swaps each 16 lanes
            // lanes[ 0:15] <-> lanes[16:31]
            // lanes[32:47] <-> lanes[48:63]
            ds_swizzle_b32 v[out_base+out_x], v[out_base+out_x] offset:0x401F
            .if elements_in_dword == 2
               ds_swizzle_b32 v[out_base+out_x+gprs_per_batch_out], v[out_base+out_x+gprs_per_batch_out] offset:0x401F
            .endif
         .elseif \swizzle==8  // swaps each 8 lanes
            // lanes[0:7] <-> lanes[8:15]
            // ...
            ds_swizzle_b32 v[out_base+out_x], v[out_base+out_x] offset:0x201F
            .if elements_in_dword == 2
               ds_swizzle_b32 v[out_base+out_x+gprs_per_batch_out], v[out_base+out_x+gprs_per_batch_out] offset:0x201F
            .endif
         .elseif \swizzle != 0
            .error "Wrong swizzle parameter"
         .endif
         out_x = out_x + 1
      .endr
   .endm


   // construct address fo ds_bpermute to swap lanes [0:31] <-> lanes [32:63]
   v_xor_b32 v[permute_addr], 0 + (1 << 5), v[tid]
   v_lshlrev_b32 v[permute_addr], 2, v[permute_addr]

loop_n_begin: // loop over batch (n)

   g_line_conv = 0
   g_line_fetch = 0
   g_line_exch = 0
   exch_line_in = 0
   exch_line_out = 0
   line_conv_in = lines_cnt_in - 1
   line_conv_out = 0
   line_fetch_in = 0
   line_fetch_out = 0

   .macro conv_filter in_line0, in_line1, in_line2, out_line, acc_batch, sync=0, swizzle=0
      static_assert (wei_h == 3)
      .if reverse_weights
         accl0 = 2
         accl1 = 1
         accl2 = 0
      .else
         accl0 = 0
         accl1 = 1
         accl2 = 2
      .endif
      .if img_h == 1
         conv_line \in_line1, \out_line, accl1, \acc_batch, \sync, \swizzle
      .elseif g_line_conv == 0
         conv_line \in_line1, \out_line, accl1, \acc_batch, \sync
         conv_line \in_line2, \out_line, accl2, \acc_batch, 0, \swizzle
      .elseif g_line_conv == img_h - 1
         conv_line \in_line0, \out_line, accl0, \acc_batch, \sync
         conv_line \in_line1, \out_line, accl1, \acc_batch, 0, \swizzle
      .else
         conv_line \in_line0, \out_line, accl0, \acc_batch, \sync
         conv_line \in_line1, \out_line, accl1, \acc_batch
         conv_line \in_line2, \out_line, accl2, \acc_batch, 0, \swizzle
      .endif
   .endm

   .macro fetch_step mbufs_inflight
      .if g_line_fetch < img_h
         .if g_line_fetch < img_h - 1
            .load_line in, line_fetch_in, \mbufs_inflight
         .else
            skipped_fetches = skipped_fetches + 1
         .endif
         .if !(g_line_fetch % stride_h)
            .load_line out, line_fetch_out, \mbufs_inflight
         .else
            skipped_fetches = skipped_fetches + 1
         .endif
      .else
         skipped_fetches = skipped_fetches + 2
      .endif
      g_line_fetch = g_line_fetch + 1
   .endm

   .macro conv_step
      .if g_line_conv < img_h
         l2 = line_conv_in
         l0 = l2
         .next_line in, l2
         l1 = l2
         .next_line in, l2
         .next_line in, line_conv_in

         .if !(g_line_conv % stride_h)
            .if k_per_wave == 1
               conv_filter l0, l1, l2, line_conv_out, 0
            .elseif k_per_wave == 2
               conv_filter l0, l1, l2, line_conv_out, 0, 0, chunk_size
               conv_filter l0, l1, l2, line_conv_out, 1, 1, 0
            .elseif k_per_wave == 4
               conv_filter l0, l1, l2, line_conv_out, 0, 0, chunk_size
               conv_filter l0, l1, l2, line_conv_out, 1, 1, chunk_size * 2
               conv_filter l0, l1, l2, line_conv_out, 2, 1, chunk_size
               conv_filter l0, l1, l2, line_conv_out, 3, 1, 0
            .elseif k_per_wave == 8
               conv_filter l0, l1, l2, line_conv_out, 0, 0, chunk_size
               conv_filter l0, l1, l2, line_conv_out, 1, 1, chunk_size * 2
               conv_filter l0, l1, l2, line_conv_out, 2, 1, chunk_size
               conv_filter l0, l1, l2, line_conv_out, 3, 1, chunk_size * 4
               conv_filter l0, l1, l2, line_conv_out, 4, 1, chunk_size
               conv_filter l0, l1, l2, line_conv_out, 5, 1, chunk_size * 2
               conv_filter l0, l1, l2, line_conv_out, 6, 1, chunk_size
               conv_filter l0, l1, l2, line_conv_out, 7, 1, 0
            .else
               static_assert(0)
            .endif
            .next_line out, line_conv_out
         .endif

      .endif

      g_line_conv = g_line_conv + 1
   .endm

   mbufs_cur = 0
   .load_line in, line_fetch_in, mbufs_cur
   s_mov_b32 s[loop_h_cnt], 0

   // pipe prologue
   skipped_fetches = 0
   fetch_step mbufs_cur
   mbufs_1row = mbufs_cur
   .rept pipe_lines_depth-1
      fetch_step mbufs_cur
   .endr
   vmcnt_per_step = (mbufs_per_line_in + mbufs_per_line_out) * elements_in_dword

   mbufs_piped = mbufs_cur - mbufs_1row
   s_wait mbufs_piped
   exch_line_f16 in
   exch_step_f16
   conv_step

   // software pipeline
loop_h_begin:
   period = lines_cnt_in * lines_cnt_out * stride_h
   unroll_factor = 1 * period
   pipelined_steps = img_h - pipe_lines_depth - 1
   .if pipelined_steps < 0
      pipelined_steps = 0
   .endif
   h_cycles = pipelined_steps / unroll_factor
   .if h_cycles > 0
      cur_conv_line = g_line_conv
      cur_fetch_line = g_line_fetch
      .rept unroll_factor
         fetch_step mbufs_cur
         s_wait mbufs_piped
         exch_step_f16
         conv_step
      .endr
      s_addk_i32 s[loop_h_cnt], 1
      s_cmpk_ge_u32 s[loop_h_cnt], 0+h_cycles
      s_cbranch_scc0 loop_h_begin
      g_line_conv = cur_conv_line + unroll_factor * h_cycles
      g_line_fetch = cur_fetch_line + unroll_factor * h_cycles
   .endif
   non_looped_pipelined_steps = pipelined_steps - unroll_factor * h_cycles
loop_h_end:

   .rept non_looped_pipelined_steps
      fetch_step mbufs_cur
      s_wait mbufs_piped
      exch_step_f16
      conv_step
   .endr

   // pipe epilogue
   fetch_step mbufs_cur
   iii = 0
   .rept pipe_lines_depth
      iii = iii + 1
      s_wait mbufs_piped - iii * vmcnt_per_step
      exch_step_f16
      conv_step
   .endr


   s_add_u32 s[soffset_in], s[soffset_in], 0 + input_stack_size * n_per_group * elements_in_dword - input_feature_map_size
   s_add_u32 s[soffset_out], s[soffset_out], 0 + output_stack_size * n_per_group * elements_in_dword - output_feature_map_size
loop_n_end:
   s_addk_i32 s[loop_n_cnt], 0 + elements_in_dword
   s_cmpk_ge_u32 s[loop_n_cnt], 0 + (batch_size + n_per_group - 1) / n_per_group
   s_cbranch_scc0 loop_n_begin

   // reduction across waves in group
   // all waves but last store accums to LDS and dies
   // last wave survives and read LDS
   .GPR_REUSE voffset_in, lds_off
   .if n_per_group > 1
      s_mov_b32 m0, -1
      s_cmpk_eq_u32 s[wave_id], 0 + n_per_group - 1
      s_cbranch_scc1 last_wave

      s_mulk_i32 s[wave_id], 4 * 64 * accums_cnt
      //v_lshlrev_b32 v[lds_off], 4, v[tid]
      //_v_add_co_u32 v[lds_off], vcc, s[wave_id], v[lds_off]
      //.ds_write_all
      v_lshlrev_b32 v[lds_off], 2, v[tid]
     _v_add_co_u32 v[lds_off], vcc, s[wave_id], v[lds_off]
      imm_off = 0
      cur_accum = accums
      .rept accums_cnt
         ds_write_b32 v[lds_off], v[cur_accum], offset:0+imm_off
         cur_accum = cur_accum + 1
         imm_off = imm_off + 4 * 64
      .endr
      s_waitcnt 0
      s_barrier
      s_endpgm
last_wave:
      s_barrier

      v_lshlrev_b32 v[lds_off], 2, v[tid]
      .rept n_per_group-1
         imm_off = 0
         cur_accum = accums
         tmp_accum = lines_in
         .rept accums_cnt
            ds_read_b32 v[tmp_accum], v[lds_off] offset:0+imm_off
            s_waitcnt 0
            v_add_f32 v[cur_accum], v[tmp_accum], v[cur_accum]
            imm_off = imm_off + 4 * 64
            cur_accum = cur_accum + 1
         .endr
        _v_add_co_u32 v[lds_off], vcc, 4 * 64 * accums_cnt, v[lds_off]
      .endr
   .endif


   // reduction inside each chunk
   s_mov_b64 exec, -1
   reduction_steps = chunk_size_log2

   static_assert(accums_cnt > 2)
   .macro reduction max_step
      .irpc step, 123456
         .if \step <= \max_step
            acc = accums
            .rept accums_cnt
               .if \step == 1
                  v_add_f32 v[acc], v[acc], v[acc] quad_perm:[1,0,3,2]
               .elseif \step == 2
                  v_add_f32 v[acc], v[acc], v[acc] quad_perm:[2,3,1,0]
               .elseif \step == 3
                  v_add_f32 v[acc], v[acc], v[acc] row_ror:12
               .elseif \step == 4
                  v_add_f32 v[acc], v[acc], v[acc] row_ror:8
               .elseif \step == 5
                  static_assert (0) //v_add_f32 v[acc], v[acc], v[acc] row_bcast:15
               .elseif \step == 6
                  static_assert (0) //v_add_f32 v[acc], v[acc], v[acc] row_bcast:31
               .endif
               acc = acc + 1
            .endr
         .endif
      .endr
   .endm


   reduction reduction_steps


   //storing result
   static_assert(chunk_size == 16 || chunk_size == 8)
   .if chunk_size == 16
      s_mov_b32 exec_hi, 0x00010001
   .else
      s_mov_b32 exec_hi, 0x01010101
   .endif
   s_mov_b32 exec_lo, exec_hi

   .GPR_REUSE lds_off, c_pattern
   .GPR_REUSE voffset_out, k_pattern
   .GPR_REUSE lines_in, voffset_wei
   .GPR_REUSE lines_out, c_offset
   .GPR_REUSE permute_addr, tid_wei

   .macro store_accums acc
      static_assert (weights_per_filter == 9)
      acc_base = accums + \acc * weights_per_filter
      .if elements_in_dword == 2
         v_cvt_pkrtz_f16_f32 v[acc_base], v[acc_base], v[acc_base+1]
         v_cvt_pkrtz_f16_f32 v[acc_base+1], v[acc_base+2], v[acc_base+3]
         v_cvt_pkrtz_f16_f32 v[acc_base+2], v[acc_base+4], v[acc_base+5]
         v_cvt_pkrtz_f16_f32 v[acc_base+3], v[acc_base+6], v[acc_base+7]
         buffer_store_dwordx4 v[acc_base:acc_base+3], v[voffset_wei], s[desc_wei:desc_wei+3], s[soffset_wei] offen offset:0
         v_cvt_f16_f32 v[acc_base+8], v[acc_base+8]
         buffer_store_short v[acc_base+8], v[voffset_wei], s[desc_wei:desc_wei+3], s[soffset_wei] offen offset:0+4*4
      .else
         buffer_store_dwordx4 v[acc_base+0:acc_base+3], v[voffset_wei], s[desc_wei:desc_wei+3], s[soffset_wei] offen offset:0
         buffer_store_dwordx4 v[acc_base+4:acc_base+7], v[voffset_wei], s[desc_wei:desc_wei+3], s[soffset_wei] offen offset:0+4*4
         buffer_store_dword v[acc_base+8], v[voffset_wei], s[desc_wei:desc_wei+3], s[soffset_wei] offen offset:0+8*4
      .endif
   .endm

   v_mbcnt_hi_u32_b32 v[tid_wei], exec_lo, 0
   v_mbcnt_lo_u32_b32 v[tid_wei], exec_hi, v[tid_wei]

   v_mov_b32 v[c_pattern], v[tid_wei]
   v_and_b32 v[k_pattern], 0 + (1 << k_per_wave_log2) - 1, v[tid_wei]
   v_mul_u32_u24 v[c_offset], 0 + filter_c_stride, v[c_pattern]

   // k_pattern sequence for all accumulators is a gray code sequence.
   // Following macro produces bit mask (\A) that is used to produce next
   // number in a gray sequence

   .macro gray_walker A, B
      .if \B == 1
         v_xor_b32 v[k_pattern], 0 + \A, v[k_pattern]
         v_mul_u32_u24 v[voffset_wei], 0 + filter_k_stride, v[k_pattern]
        _v_add_co_u32 v[voffset_wei], vcc, v[voffset_wei], v[c_offset]
         store_accums curr_accum
         curr_accum = curr_accum + 1
      .else
         gray_walker \A, \B/2
         gray_walker \B/2, \B/2
      .endif
   .endm

   curr_accum = 0
   gray_walker 0, k_per_wave

s_endpgm

.Lfunc_end0:
   .size gcnAsmConv3x3WrW, .Lfunc_end0 - gcnAsmConv3x3WrW


.ifndef ROCM_METADATA_VERSION
.error "ROCM_METADATA_VERSION must be defined"
.endif

.macro METADATA wg_x, lds_size
  .if ROCM_METADATA_VERSION == 4
    .amd_amdgpu_hsa_metadata
    { Version: [ 1, 0 ],
        Kernels:
        - { Name: gcnAsmConv3x3WrW, SymbolName: 'gcnAsmConv3x3WrW@kd', Language: OpenCL C, LanguageVersion: [ 1, 2 ],
            Attrs:
              { ReqdWorkGroupSize: [ \wg_x, 1, 1 ] }
            CodeProps:
              { KernargSegmentSize: 64, GroupSegmentFixedSize: \lds_size, PrivateSegmentFixedSize: 0, KernargSegmentAlign: 8, WavefrontSize: 64, MaxFlatWorkGroupSize: \wg_x }
            Args:
            - { Name: N       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: C       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: H       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: W       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: K       , Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: n_groups, Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: unused_0, Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: unused_1, Size: 4, Align: 4, ValueKind: ByValue, ValueType: I32, TypeName: 'int', AccQual: Default, IsConst: true }
            - { Name: x       , Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: F32, TypeName: 'float*', AddrSpaceQual: Global, AccQual: Default, IsConst: true }
            - { Name: dw      , Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: F32, TypeName: 'float*', AddrSpaceQual: Global, AccQual: Default }
            - { Name: dy      , Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: F32, TypeName: 'float*', AddrSpaceQual: Global, AccQual: Default, IsConst: true }
            - { Name: ret_addr, Size: 8, Align: 8, ValueKind: GlobalBuffer, ValueType: I32, TypeName: 'int*'  , AddrSpaceQual: Global, AccQual: Default }
          }
    }
    .end_amd_amdgpu_hsa_metadata
  .else
    .error "Unsupported ROCM_METADATA_VERSION"
    .end
  .endif
.endm

workgroup_size_x = n_per_group * 64

.altmacro
.macro METADATA_WRAPPER wg_x, lds_size
    METADATA %\wg_x, %\lds_size
.endm

METADATA_WRAPPER workgroup_size_x, .AUTO_LDS_BYTE_SIZE
      /*******************************************************************************
 *
 * MIT License
 *
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 *******************************************************************************/

.hsa_code_object_version 2,1
.hsa_code_object_isa

.text
.globl gcnAsmConv1x1WrW
.p2align 8
.type gcnAsmConv1x1WrW,@function
.amdgpu_hsa_kernel gcnAsmConv1x1WrW

/*******************************************************************************
 *
 * MIT License
 *
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 *******************************************************************************/

.set __auto_gpr_count_guard, 1

.set .WAVE_SIZE, 64
.set .WAVE_SIZE_LOG2, 6
.set .MAX_VGPRS, 256
.set .MAX_SGPRS, 102
.set .MAX_LDS, 65536

.macro .GPR_ALLOC_BEGIN
    .set .AVAILABLE_VGPRS, .MAX_VGPRS
    .set .AVAILABLE_SGPRS, .MAX_SGPRS
	.set .AVAILABLE_LDS, .MAX_LDS
	.set .SGPR_NEXT_FREE, 0
	.set .VGPR_NEXT_FREE, 0
	.set .LDS_NEXT_FREE, 0
	.set .AUTO_VGPR_COUNT, 0
	.set .AUTO_SGPR_COUNT, 0
	.set .AUTO_LDS_BYTE_SIZE, 0
.ifnotdef EXPERIMENTAL_COv3	
	.set .AUTO_VGPR_GRANULATED_COUNT, 0
	.set .AUTO_SGPR_GRANULATED_COUNT, 0
.endif	
	.set __sgpr_reserve_vcc, 0
	.set __sgpr_reserve_xnack, 0
	.set __sgpr_reserve_flatscr, 0
	.set __auto_gpr_count_guard, 0
	.set __max_waves_limit, 10
	.set __min_waves_limit, 1
.endm

.macro .CHECK_SGPR_ALLOCATION gprs_to_allocate=0
	.if (.SGPR_NEXT_FREE + \gprs_to_allocate) > .AVAILABLE_SGPRS
		.error "Error: out of free sgprs"
        .end
	.endif
.endm

.macro .CHECK_VGPR_ALLOCATION gprs_to_allocate=0
	.if (.VGPR_NEXT_FREE + \gprs_to_allocate) > .AVAILABLE_VGPRS
		.error "Error: out of free vgprs"
        .end
	.endif
.endm

.macro .CHECK_LDS_ALLOCATION bytes_to_allocate=0
	.if (.LDS_NEXT_FREE + \bytes_to_allocate) > .AVAILABLE_LDS
		.error "Error: out of free lds"
        .end
	.endif
.endm

.macro .GPRS_FOR_WAVE_LIMIT waves_per_simd, sgprs, vgprs
    .if \waves_per_simd == 10
	    \sgprs = 80
		\vgprs = 24
	.elseif \waves_per_simd == 9
	    \sgprs = 96
		\vgprs = 28
	.elseif \waves_per_simd == 8
	    \sgprs = 96
		\vgprs = 32
	.elseif \waves_per_simd == 7
	    \sgprs = 102
		\vgprs = 36
	.elseif \waves_per_simd == 6
	    \sgprs = 102
		\vgprs = 40
	.elseif \waves_per_simd == 5
	    \sgprs = 102
		\vgprs = 48
	.elseif \waves_per_simd == 4
	    \sgprs = 102
		\vgprs = 64
	.elseif \waves_per_simd == 3
	    \sgprs = 102
		\vgprs = 84
	.elseif \waves_per_simd == 2
	    \sgprs = 102
		\vgprs = 128
	.else
	    \sgprs = 102
		\vgprs = 256
	.endif
.endm

.macro .SET_MIN_WAVES_LIMIT waves_per_simd
    .if \waves_per_simd > 10
		.error "Error: max 10 waves per simd is available"
	.endif
	.GPRS_FOR_WAVE_LIMIT \waves_per_simd, .AVAILABLE_SGPRS, .AVAILABLE_VGPRS
	.CHECK_SGPR_ALLOCATION
	.CHECK_VGPR_ALLOCATION
	__min_waves_limit = \waves_per_simd
	.if __min_waves_limit > __max_waves_limit
	    .error "Error: __min_waves_limit > __max_waves_limit"
	.endif
.endm

.macro .SET_MAX_WAVES_LIMIT waves_per_simd
    .if \waves_per_simd < 1
		.error "Error: waves per simd should be > 0"
	.endif
	__max_waves_limit = \waves_per_simd
	.if __min_waves_limit > __max_waves_limit
	    .error "Error: __min_waves_limit > __max_waves_limit"
	.endif
.endm


.macro .GPR_ALLOC_END
    .if __auto_gpr_count_guard == 1
	    .error "Error: unpaired .GPR_ALLOC_END. Please invoke .GPR_ALLOC_BEGIN before each kernel."
	.endif
	.CHECK_SGPR_ALLOCATION
	.CHECK_VGPR_ALLOCATION
	.CHECK_LDS_ALLOCATION
	__sgpr_additional_count = 2 * (__sgpr_reserve_flatscr + __sgpr_reserve_xnack + __sgpr_reserve_vcc)
	.GPRS_FOR_WAVE_LIMIT __max_waves_limit, .AUTO_SGPR_COUNT, .AUTO_VGPR_COUNT
	.if .AUTO_VGPR_COUNT < .VGPR_NEXT_FREE
	    .AUTO_VGPR_COUNT = .VGPR_NEXT_FREE
	.endif
	.if .AUTO_SGPR_COUNT < (.SGPR_NEXT_FREE + __sgpr_additional_count)
	    .AUTO_SGPR_COUNT = (.SGPR_NEXT_FREE + __sgpr_additional_count)
	.endif
.ifnotdef EXPERIMENTAL_COv3	
	.AUTO_VGPR_GRANULATED_COUNT = (.AUTO_VGPR_COUNT - 1)/4
	.AUTO_SGPR_GRANULATED_COUNT = (.AUTO_SGPR_COUNT - 1)/8
.endif	
	.AUTO_LDS_BYTE_SIZE = .LDS_NEXT_FREE
    __auto_gpr_count_guard = 1
.endm

.macro .VGPR_ALLOC_FROM __vgpr_alloc_from
    .set .VGPR_NEXT_FREE, \__vgpr_alloc_from
.endm

.macro .SGPR_ALLOC_FROM __sgpr_alloc_from
    .set .SGPR_NEXT_FREE, \__sgpr_alloc_from
.endm

.macro .LDS_ALLOC_FROM __lds_alloc_from
	.set .LDS_NEXT_FREE, \__lds_alloc_from
.endm

.macro .SGPR_RESERVE_FLATSCR
    .set __sgpr_reserve_flatscr, 1
.endm

.macro .SGPR_RESERVE_XNACK
    .set __sgpr_reserve_xnack, 1
.endm

.macro .SGPR_RESERVE_VCC
    .set __sgpr_reserve_vcc, 1
.endm

.macro .VGPR_ALLOC __vgpr_number_symbolic, __vgpr_numregs=1
    .CHECK_VGPR_ALLOCATION \__vgpr_numregs
    .set \__vgpr_number_symbolic, .VGPR_NEXT_FREE
    .set .VGPR_NEXT_FREE, .VGPR_NEXT_FREE + \__vgpr_numregs
.endm

.macro .SGPR_ALLOC __sgpr_number_symbolic, __sgpr_numregs=1, __sgpr_alignment=0
    .CHECK_SGPR_ALLOCATION \__sgpr_numregs
	.if \__sgpr_alignment > 0
		.set __sgpr_effective_alignment, \__sgpr_alignment
	.elseif \__sgpr_numregs > 4
		.set __sgpr_effective_alignment, 4
	.else
		.set __sgpr_effective_alignment, \__sgpr_numregs
	.endif
    .if .SGPR_NEXT_FREE % __sgpr_effective_alignment != 0
		.error "Error: unaligned register"
    .endif
    .set \__sgpr_number_symbolic, .SGPR_NEXT_FREE
    .set .SGPR_NEXT_FREE, .SGPR_NEXT_FREE + \__sgpr_numregs
.endm

.macro .LDS_ALLOC __lds_ptr_name, __bytes_to_allocate, __alignment_size=1
	.if .LDS_NEXT_FREE % \__alignment_size != 0
		.LDS_ALLOC_FROM ((.LDS_NEXT_FREE / \__alignment_size) + 1) * \__alignment_size
	.endif
	.CHECK_LDS_ALLOCATION \__bytes_to_allocate
	.set \__lds_ptr_name, .LDS_NEXT_FREE
	.set .LDS_NEXT_FREE, .LDS_NEXT_FREE + \__bytes_to_allocate
.endm

.macro .SGPR_ALLOC_ONCE __sgpr_symbolic, __sgpr_numregs=1, __sgpr_alignment=0
	.ifndef __guard_sgpr_\__sgpr_symbolic
		__guard_sgpr_\__sgpr_symbolic = 0
	.endif
	.if __guard_sgpr_\__sgpr_symbolic == 0
		__guard_sgpr_\__sgpr_symbolic = 1
		.SGPR_ALLOC \__sgpr_symbolic, \__sgpr_numregs, \__sgpr_alignment
	.endif
.endm

.macro .GPR_INVALIDATE __gpr_symbolic
	.set \__gpr_symbolic, 0x7fffffff /* invalidate (intentionally to the middle of the int range) */
.endm

.macro .GPR_REUSE __gpr_number_symbolic_old, __gpr_number_symbolic_new
    .set \__gpr_number_symbolic_new, \__gpr_number_symbolic_old
    .GPR_INVALIDATE \__gpr_number_symbolic_old
.endm

/*******************************************************************************
 *
 * MIT License
 *
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 *******************************************************************************/

.ifdef EXPERIMENTAL_COv3
    .ifdef .option.machine_version_major
        .error ".option.machine_version_major already defined"
        .end
    .endif
    .set .option.machine_version_major, .amdgcn.gfx_generation_number
.endif

LAYOUT_DATA_NCHW = 0
LAYOUT_DATA_CNHW = 1
LAYOUT_DATA_NHWC = 2
LAYOUT_DATA_CHWN = 3

// acc_type and buf_type: 0 - FP64, 1 - FP32, 2 - FP16, 5 - int32, 6 - int16, 7 - int8
TYPE_FP64  = 0
TYPE_FP32  = 1
TYPE_FP16  = 2
TYPE_BFP16 = 3
TYPE_INT64 = 4
TYPE_INT32 = 5
TYPE_INT16 = 6
TYPE_INT8  = 7
TYPE_INT4  = 8

.macro log2 lg2, num, max_bits=8
    \lg2 = 0
    lg_i = \num
    .rept \max_bits
        lg_i = lg_i / 2
        .if lg_i > 0
            \lg2 = \lg2 + 1
        .endif
    .endr
.endm

.macro default symbol, value
    .ifnotdef \symbol
        \symbol = \value
    .endif
.endm

.macro static_assert fufufu
    .if !\fufufu
        .error "\fufufu is false"
        .end
    .endif
.endm

.macro swap a, b
    __tmp = \a
    \a = \b
    \b = __tmp
.endm

.macro m_bpermute vgpr, cnt, addr
    v = \vgpr
    .rept \cnt
        ds_bpermute_b32 v[v], v[\addr], v[v]
        v = v + 1
    .endr
.endm

.macro m_swizzle vgpr, cnt, pattern
    v = \vgpr
    .rept \cnt
        ds_swizzle_b32 v[v], v[v] offset:\pattern
        v = v + 1
    .endr
.endm

.if (.option.machine_version_major == 8)
    .set max_hw_vctn, 15
.elseif (.option.machine_version_major == 9)
    .set max_hw_vctn, 63
.endif
max_hw_lcnt = 15
.macro s_wait vmcnt=max_hw_vctn, lgkmcnt=max_hw_lcnt
    vm_cnt = \vmcnt
    lgkm_cnt = \lgkmcnt
    .if vm_cnt > max_hw_vctn
        vm_cnt = max_hw_vctn
    .elseif vm_cnt < 0
        vm_cnt = 0
    .endif
    .if lgkm_cnt > max_hw_lcnt
        lgkm_cnt = max_hw_lcnt
    .elseif lgkm_cnt < 0
        lgkm_cnt = 0
    .endif
    s_waitcnt vmcnt(0 + vm_cnt) & lgkmcnt(0 + lgkm_cnt)
.endm


maxU24 = 1 << 24

wave_size = 64
log2 wave_size_log2, wave_size

.macro m_buffer_load_dwordx size, dst, off, desc, soff, ioff=0
    .if \size == 1
        buffer_load_dword v[\dst], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .elseif \size == 2
        buffer_load_dwordx2 v[\dst:\dst+\size-1], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .elseif \size == 3
        buffer_load_dwordx3 v[\dst:\dst+\size-1], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .elseif \size == 4
        buffer_load_dwordx4 v[\dst:\dst+\size-1], v[\off], s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .endif
.endm

.macro m_buffer_load_ushort size, dst, off, desc, soff, ioff=0
    .if \size == 1
        buffer_load_ushort v[\dst], v[\off],  s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .endif
.endm

.macro m_buffer_store_short size, src, off, desc, soff, ioff=0
    .if \size == 1
        buffer_store_short v[\src], v[\off],  s[\desc:\desc + 3], s[\soff] offen offset:0+\ioff
    .endif
.endm

.macro u32_div numer, denom, uquo, v_tmp, s_tmp
    u_div   v[\numer], v[\denom] v[\uquo], \v_tmp, \s_tmp, \s_tmp + 2 
.endm


// Unsigned division function from the SC implementation
// 4 s_tmps, 4 v_tmps
//
.macro u_div numer, denom, uquo, vtmp, stmp1, stmp2
    v_cvt_f32_u32     v[\vtmp],     \denom
    v_rcp_f32         v[\vtmp],     v[\vtmp]
    v_mul_f32         v[\vtmp],     0x4f800000,   v[\vtmp]
    v_cvt_u32_f32     v[\vtmp],     v[\vtmp]

    v_mul_lo_u32      v[\vtmp+1],   \denom,       v[\vtmp]
    v_mul_hi_u32      v[\vtmp+2],   \denom,       v[\vtmp]
   _v_sub_co_u32      v[\vtmp+3],   vcc,          0,           v[\vtmp+1]
    v_cmp_ne_i32      s[\stmp1:\stmp1+1], 0,      v[\vtmp+2]
    v_cndmask_b32     v[\vtmp+1],   v[\vtmp+3],   v[\vtmp+1],  s[\stmp1:\stmp1+1]
    v_mul_hi_u32      v[\vtmp+1],   v[\vtmp+1],   v[\vtmp]
   _v_sub_co_u32      v[\vtmp+2],   vcc,          v[\vtmp],    v[\vtmp+1]
    v_add_co_u32      v[\vtmp],     vcc,          v[\vtmp],    v[\vtmp+1]
    v_cndmask_b32     v[\vtmp],     v[\vtmp],     v[\vtmp+2],  s[\stmp1:\stmp1+1]
    v_mul_hi_u32      v[\vtmp],     v[\vtmp],     \numer
    v_mul_lo_u32      v[\vtmp+1],   v[\vtmp],     \denom
   _v_sub_co_u32      v[\vtmp+2],   vcc,          \numer,      v[\vtmp+1]
    v_cmp_ge_u32      s[\stmp1:\stmp1+1],         \numer,      v[\vtmp+1]
    v_cmp_ge_u32      s[\stmp2:\stmp2+1],         v[\vtmp+2],  \denom
   _v_add_co_u32      v[\vtmp+2],   vcc,          1,           v[\vtmp]
    s_and_b64         s[\stmp2:\stmp2+1], s[\stmp1:\stmp1+1],  s[\stmp2:\stmp2+1]
   _v_add_co_u32      v[\vtmp+1],   vcc, -1,      v[\vtmp]
    v_cndmask_b32     v[\vtmp+2],   v[\vtmp],     v[\vtmp+2],  s[\stmp2:\stmp2+1]
    v_cndmask_b32     v[\vtmp+2],   v[\vtmp+1],   v[\vtmp+2],  s[\stmp1:\stmp1+1]
    v_cmp_ne_i32      vcc,          0,            \denom
    v_cndmask_b32     \uquo,        -1,           v[\vtmp+2],  vcc
.endm

.altmacro
.macro ceil_2_32_div_u16 m, denom, vtmp, stmp
	v_cvt_f32_u32     v[\vtmp],     \denom
	v_rcp_f32         v[\vtmp],     v[\vtmp]
	v_mul_f32         v[\vtmp],     0x4f800000,   v[\vtmp]
	v_cvt_u32_f32     v[\vtmp],     v[\vtmp]

	v_mul_lo_u32      v[\vtmp+1],   \denom,       v[\vtmp]
	v_mul_hi_u32      v[\vtmp+2],   \denom,       v[\vtmp]
	v_sub_u32         v[\vtmp+3],   0,            v[\vtmp+1]
	v_cmp_ne_i32      s[\stmp:\stmp+1], 0,        v[\vtmp+2]
	v_cndmask_b32     v[\vtmp+1],   v[\vtmp+3],   v[\vtmp+1],  s[\stmp:\stmp+1]
	v_mul_hi_u32      v[\vtmp+1],   v[\vtmp+1],   v[\vtmp]
	v_sub_u32         v[\vtmp+2],   v[\vtmp],     v[\vtmp+1]
	v_add_co_u32      v[\vtmp],     vcc,          v[\vtmp],    v[\vtmp+1]
	v_cndmask_b32     v[\vtmp],     v[\vtmp],     v[\vtmp+2],  s[\stmp:\stmp+1]
	v_mul_hi_u32      v[\vtmp],     -1,           v[\vtmp]
	v_mul_lo_u32      v[\vtmp+1],   v[\vtmp],     \denom
	v_sub_u32         v[\vtmp+2],   -1,           v[\vtmp+1]
	v_cmp_ge_u32      s[\stmp:\stmp+1],           v[\vtmp+2],  \denom
	v_add_u32         v[\vtmp+2],   1,            v[\vtmp]
	v_add_co_u32      v[\vtmp+1],   vcc, -1,      v[\vtmp]
	v_cndmask_b32     v[\vtmp+2],   v[\vtmp],     v[\vtmp+2],  s[\stmp:\stmp+1]
	v_add_u32         v[\vtmp+2],   1,            v[\vtmp+2]
	v_cmp_ne_i32      vcc,          0,            \denom
	v_cndmask_b32     \m,        -1,           v[\vtmp+2],  vcc
.endm

.macro disable_srd srd
	s_mov_b32 s[\srd+3], 0
.endm
.macro enable_srd srd
	s_mov_b32 s[\srd+3], 0x00020000            // DATA_FORMAT, need to just be non-zero;
.endm

.macro label l, n
	\l\n:
.endm
.macro _s_cbranch cond, l, n
	s_cbranch_\cond \l\n
.endm
.macro _s_branch l, n
	s_branch \l\n
.endm


div_const_1_2=0x80000000
div_const_1_3=0x55555556
div_const_1_4=0x40000000
div_const_1_5=0x33333334
div_const_1_6=0x2aaaaaab
div_const_1_7=0x24924925
div_const_1_8=0x20000000
div_const_1_9=0x1c71c71d
div_const_1_10=0x1999999a
div_const_1_11=0x1745d175
div_const_1_12=0x15555556
div_const_1_13=0x13b13b14
div_const_1_14=0x12492493
div_const_1_15=0x11111112
div_const_1_16=0x10000000
div_const_1_17=0x0f0f0f10
div_const_1_18=0x0e38e38f
div_const_1_19=0x0d79435f
div_const_1_20=0x0ccccccd
div_const_1_21=0x0c30c30d
div_const_1_22=0x0ba2e8bb
div_const_1_23=0x0b21642d
div_const_1_24=0x0aaaaaab
div_const_1_25=0x0a3d70a4
div_const_1_26=0x09d89d8a
div_const_1_27=0x097b425f
div_const_1_28=0x0924924a
div_const_1_29=0x08d3dcb1
div_const_1_30=0x08888889
div_const_1_31=0x08421085
div_const_1_32=0x08000000
div_const_1_33=0x07c1f07d
div_const_1_34=0x07878788
div_const_1_35=0x07507508
div_const_1_36=0x071c71c8
div_const_1_37=0x06eb3e46
div_const_1_38=0x06bca1b0
div_const_1_39=0x06906907
div_const_1_40=0x06666667
div_const_1_41=0x063e7064
div_const_1_42=0x06186187
div_const_1_43=0x05f417d1
div_const_1_44=0x05d1745e
div_const_1_45=0x05b05b06
div_const_1_46=0x0590b217
div_const_1_47=0x0572620b
div_const_1_48=0x05555556
div_const_1_49=0x0539782a
div_const_1_50=0x051eb852
div_const_1_51=0x05050506
div_const_1_52=0x04ec4ec5
div_const_1_53=0x04d4873f
div_const_1_54=0x04bda130
div_const_1_55=0x04a7904b
div_const_1_56=0x04924925
div_const_1_57=0x047dc120
div_const_1_58=0x0469ee59
div_const_1_59=0x0456c798
div_const_1_60=0x04444445
div_const_1_61=0x04325c54
div_const_1_62=0x04210843
div_const_1_63=0x04104105
div_const_1_64=0x04000000

.macro _s_div_const_u32_u16 dst, src, denum
	.if \denum == 1
		s_mov_b32 \dst, \src
	.elseif \denum >=2 && \denum <= 64
		s_mul_hi_u32 \dst, div_const_1_\denum, \src
	.else
		static_assert(0)
	.endif
.endm

.macro _v_div_const_u32_u16 dst, src, denum, tmp
	.if \denum == 1
		v_mov_b32 \dst, \src
	.elseif \denum >=2 && \denum <= 64
		s_mov_b32 \tmp, div_const_1_\denum
		v_mul_hi_u32 \dst, \tmp, \src
	.else
		static_assert(0)
	.endif
.endm

.macro _s_ceil_u32 dst, src, denum
	s_add_u32 \dst, \denum - 1, \src
	_s_div_const_u32_u16 \dst, \dst, \denum
.endm

/*******************************************************************************
 * 
 * MIT License
 * 
 * Copyright (c) 2017 Advanced Micro Devices, Inc.
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 * 
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 * 
 *******************************************************************************/

// Wrapper macros for some instructions.
// Macros contain workarounds for some assembler bugs.
// Also these allow for unifying source text when different
// ISA versions require different assembler syntax (mostly mnemonics).

.if ((.option.machine_version_major == 8) || (.option.machine_version_major == 9))
.else
    .error "Only Gfx8 and Gfx9 ISA is supported"
    .end
.endif

.ifndef WORKAROUND_BUG_34765
    .set WORKAROUND_BUG_34765,0
.endif

// Let's use Gfx10-like naming conventions for wrapper macros.
// ADD_NC
.macro _v_add_nc_u32 dst, src0, src1, dpp=
    .if (.option.machine_version_major == 8)
        // None No-Carry instruction in Gfx8, modifies VCC.
        v_add_u32 \dst, vcc, \src0, \src1 \dpp
    .else
        v_add_u32 \dst, \src0, \src1 \dpp
    .endif
.endm

// SUB_NC
.macro _v_sub_nc_u32 dst, src0, src1, dpp=
    .if (.option.machine_version_major == 8)
        // None No-Carry instruction in Gfx8, modifies VCC.
        v_sub_u32 \dst, vcc, \src0, \src1 \dpp
    .else
        v_sub_u32 \dst, \src0, \src1 \dpp
    .endif
.endm

// ADD_CO (gfx8 add)
.macro _v_add_co_u32 dst, co, src0, src1, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
        v_add_u32 \dst, \co, \src0, \src1 \dpp
    .else
        v_add_co_u32 \dst, \co, \src0, \src1 \dpp
    .endif
.endm

// ADD_CO_CI (gfx8 addc)
.macro _v_add_co_ci_u32 dst, co, src0, src1, ci, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
		v_addc_u32 \dst, \co, \src0, \src1, \ci \dpp
    .else
		v_addc_co_u32 \dst, \co, \src0, \src1, \ci \dpp
    .endif
.endm

// SUB_CO (gfx8 sub)
.macro _v_sub_co_u32 dst, co, src0, src1, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
        v_sub_u32 \dst, \co, \src0, \src1 \dpp
    .else
        v_sub_co_u32 \dst, \co, \src0, \src1 \dpp
    .endif
.endm

// SUBREV_CO (gfx8 subrev)
.macro _v_subrev_co_u32 dst, co, src0, src1, dpp=
    .if ((.option.machine_version_major == 8) || ((.option.machine_version_major == 9) && (WORKAROUND_BUG_34765 == 1)))
        v_subrev_u32 \dst, \co, \src0, \src1 \dpp
    .else
        v_subrev_co_u32 \dst, \co, \src0, \src1 \dpp
    .endif
.endm

.macro v_dot2 acc, in, out
    .if (.option.machine_version_major == 9) && (.option.machine_version_minor == 0) && (.option.machine_version_stepping == 6)
        v_dot2_f32_f16 v[\acc], v[\in], v[\out], v[\acc]
    .elseif (.option.machine_version_major == 9) && (.option.machine_version_stepping > 2)
        v_fma_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
        v_fma_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[1,1,0] op_sel_hi:[1,1,0]
    .else
        v_mad_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
        v_mad_mix_f32 v[\acc], v[\in], v[\out], v[\acc] op_sel:[1,1,0] op_sel_hi:[1,1,0]
    .endif
.endm




// initial state (s[0:4] are overlapped with filtersA):
// s[0:1] - kernarg address
// s2 - wg x (none), wave_id bits: ck_id, n_id
// s3 - wg y (C)
// s4 - wg z (K)
kernarg = 0
gid_x = 2
gid_y = 3
gid_z = 4

// kernarg layout:
// dwords 0:4 - n, c, H, W, k
// dwords 5:7 - not used
// dwords 8:9 - input buffer pointer
// dwords 10:11 - weights pointer
// dwords 12:13 - output buffer pointer
.set in_ptr_off, 0x20
.set wei_ptr_off, 0x28
.set out_ptr_off, 0x30

.ifnotdef do_not_use_default_perf_params
    default n_per_gpr, 4 // 1..4, 2^n
    default c_per_gpr, 4 // 1..16, 2^n
    default c_mult, 2 // 1..16, 2^n
    default k_per_gpr, 4 // 1..16, 2^n
    default k_mult, 2 // 1..16, 2^n
    default read_size, 1 // 1..4
    default chunk_size, 4 // 1..16, 2^n
    default n_part_cnt, 1 //1..8
.endif
default limit_wave_cnt, 0
default hw_per_gpr, 1 // 1..4, 2^n
default short_store, 0
default data_prefetch, 0


group_size = n_part_cnt
static_assert (pad_h == 0 && pad_w == 0)
static_assert (stride_h == 1) // || stride_h == 2)
static_assert (stride_w == 1) // || stride_w == 2)
static_assert (wei_h == 1 && wei_w == 1)
static_assert (1 <= group_size && group_size <= 8)
static_assert (c_per_gpr * chunk_size >= 16)
static_assert (chunk_size == 1 || c_per_gpr * chunk_size <= 16) // todo: remove restriction
static_assert (c_per_gpr * n_per_gpr * hw_per_gpr * chunk_size == wave_size)
static_assert (k_per_gpr * n_per_gpr * hw_per_gpr * chunk_size <= wave_size)

static_assert ((.option.machine_version_major == 8) || (.option.machine_version_major == 9))
static_assert (filter_c_stride < maxU24)
static_assert (filter_k_stride < maxU24)
static_assert (input_c_stride < maxU24)
static_assert (output_k_stride < maxU24)

lds_element_stride = 4
dword_size = 4

elements_in_dword = 1
.if(buf_type == TYPE_FP16 || buf_type == TYPE_INT16 || buf_type == TYPE_BFP16)
    elements_in_dword = 2
.elseif(buf_type == TYPE_INT8)
    elements_in_dword = 4
.elseif(buf_type == TYPE_INT4)
    elements_in_dword = 8
.endif

elements_per_lane = read_size * elements_in_dword

is_required_sequential_c_channels = ((weights_layout == LAYOUT_DATA_NCHW) || (weights_layout == LAYOUT_DATA_NHWC)) && !short_store
is_required_sequential_k_channels = ((weights_layout == LAYOUT_DATA_CNHW) || (weights_layout == LAYOUT_DATA_CHWN)) && !short_store
sequential_c_channels = 1
sequential_k_channels = 1

.if(is_required_sequential_c_channels)
    sequential_c_channels = elements_in_dword
.elseif(is_required_sequential_k_channels)
    sequential_k_channels = elements_in_dword
.endif
static_assert(c_mult % sequential_c_channels == 0)
static_assert(k_mult % sequential_k_channels == 0)
static_assert(input_channels % sequential_c_channels == 0)
static_assert(output_channels % sequential_k_channels == 0)

bfp16_native_support = 0
dot_instructions_available = 0
.if (.option.machine_version_major == 9) && (.option.machine_version_minor == 0) && (.option.machine_version_stepping == 6)
    dot_instructions_available = 1
.endif
madmix_instructions_available = 0
fmamix_instructions_available = 0
madmix_fmamix_with_dpp_available = 0
.if (.option.machine_version_major == 9)
    .if(.option.machine_version_stepping > 2)
        fmamix_instructions_available = 1
    .else
        madmix_instructions_available = 1
    .endif
.endif

bit_convert_mult = 0
.if(buf_type == TYPE_BFP16 && !bfp16_native_support)
    bit_convert_mult = 1
.endif


log2 c_per_gpr_log2, c_per_gpr
log2 k_per_gpr_log2, k_per_gpr
log2 n_per_gpr_log2, n_per_gpr
log2 hw_per_gpr_log2, hw_per_gpr

// chunk parameters
c_quads = c_per_gpr * chunk_size / 16
.if k_per_gpr > c_quads
    k_ds_rotates = c_quads
    k_dpp_rotates = k_per_gpr / c_quads
.else
    k_ds_rotates = k_per_gpr
    k_dpp_rotates = 1
.endif
max_per_read = read_size * elements_in_dword
//max dwords per vector read

part1_chunks = max_per_read + 1
part2_chunks = max_per_read + 1
active_lanes_in_part1_chunks = 0
active_lanes_in_part2_chunks = 0

metachunk_size = chunk_size * hw_per_gpr // hw pieces are not contiguous in vgpr
log2 chunk_size_log2, chunk_size
log2 meatchunk_size_log2, metachunk_size
out_wh = out_w * out_h
full_chunk_reads = out_wh / elements_per_lane

full_reads_per_lane = full_chunk_reads / metachunk_size
partial_chunks = out_wh - full_reads_per_lane * metachunk_size * elements_per_lane
full_reads = full_reads_per_lane
full_chunks = full_reads * elements_per_lane
.if(full_reads_per_lane == 0)
    i_cnt = 1
.else
    i_cnt = ((2 * metachunk_size) * max_per_read - partial_chunks + full_reads_per_lane * elements_per_lane - 1) / (full_reads_per_lane* elements_per_lane)
    .if(i_cnt > metachunk_size)
        i_cnt = metachunk_size
    .elseif (i_cnt < 0)
        i_cnt = 0
        static_assert(0)
    .endif
.endif

i = 0
x = 0

.if(partial_chunks)
.rept i_cnt
    partial_points1 = full_reads_per_lane * elements_per_lane * i + partial_chunks

    x = max_per_read
    .rept max_per_read
        j = (partial_points1) / x
        .if(j > metachunk_size)
            j = metachunk_size
        .endif

        .if(j > 0)
            .if ( (partial_points1 % j == 0) && (x * j == partial_points1) )
                .if (part1_chunks + part2_chunks > x)
                    part1_chunks = x
                    active_lanes_in_part1_chunks = j
                    part2_chunks = 0
                    active_lanes_in_part2_chunks = 0
                    rem_lanes_ful = metachunk_size - i
                .endif
            .else
                .if ( (x % elements_in_dword == 0) )//&&  (x <= max_per_read))
                    partial_points2 = partial_points1 - x * j
                    max_per_read2 = max_per_read - x
                        y = 1
                        .rept max_per_read2
                            k = partial_points2 / y
                            .if(y * k == partial_points2 && k <= metachunk_size)
                                .if (partial_points2 % k == 0 && (y <= max_per_read2) && (part1_chunks + part2_chunks > x + y))
                                        part1_chunks = x
                                        active_lanes_in_part1_chunks = j
                                        part2_chunks = y
                                        active_lanes_in_part2_chunks = k
                                        rem_lanes_ful = metachunk_size - i
                                .endif
                            .endif ///(y * k == partial_points2)
                            y = y + 1
                        .endr
                .endif //x % elements_in_dword == 0
            .endif //partial_points1 % j == 0
        .endif
        x = x - 1
    .endr
    i = i + 1
.endr
.endif
adv_perf_param_comb = 0
.if(partial_chunks == 0)
    part1_chunks = 0
    part2_chunks = 0
    active_lanes_in_part1_chunks = 0
    active_lanes_in_part2_chunks = 0
    rem_lanes_ful = metachunk_size
.else

    .if( part2_chunks == max_per_read + 1 && part1_chunks == max_per_read + 1)
        adv_perf_param_comb = 1
        x = max_per_read
        .rept read_size
            j = (partial_chunks + x - 1)/ x
            .if( (j <= metachunk_size) && (j * x > partial_chunks) && ((j - 1) * x < partial_chunks) )
                part1_chunks = x
                active_lanes_in_part1_chunks = j - 1
                part2_chunks = 0
                active_lanes_in_part2_chunks = 0
                rem_lanes_ful = metachunk_size
            .endif
            x = x - 1
        .endr
        bound_elements_cnt = partial_chunks - part1_chunks * (active_lanes_in_part1_chunks)
    .else
        static_assert (part1_chunks * active_lanes_in_part1_chunks + part2_chunks * active_lanes_in_part2_chunks +  full_reads_per_lane * rem_lanes_ful * elements_per_lane == out_wh)
    .endif

    static_assert(part2_chunks <= max_per_read)
    static_assert(part1_chunks <= max_per_read)


    static_assert(part1_chunks + part2_chunks <= max_per_read)
    static_assert(part1_chunks > 0)
.endif
partial_chunks = (part1_chunks+ part2_chunks)
active_lanes_in_full_chunks = rem_lanes_ful



part2_offset = part1_chunks * input_w_stride * active_lanes_in_part1_chunks

.GPR_ALLOC_BEGIN
.if limit_wave_cnt
    .SET_MAX_WAVES_LIMIT limit_wave_cnt
.endif

.SGPR_ALLOC_FROM 5
.SGPR_ALLOC soffset_in
.SGPR_ALLOC soffset_out
.SGPR_ALLOC soffset_wei
.SGPR_ALLOC desc_in, 4 // input buffer descriptor
.SGPR_ALLOC desc_out, 4 // weights buffer descriptor
.SGPR_ALLOC desc_wei, 4 // output buffer descriptor
.SGPR_ALLOC bound_lanes_exec, 2
.SGPR_ALLOC loop_n_cnt
.SGPR_ALLOC loop_hw_cnt
.SGPR_ALLOC c_base
.SGPR_ALLOC k_base
.SGPR_ALLOC n_base
.SGPR_ALLOC stmp
.SGPR_ALLOC loop_begin_ptr, 2
.SGPR_ALLOC wave_id // wave_id in group

.SGPR_RESERVE_XNACK

.VGPR_ALLOC_FROM 0
.VGPR_ALLOC tid
.VGPR_ALLOC voffset_in
.VGPR_ALLOC voffset_out
.VGPR_ALLOC voffset_part1_in
.VGPR_ALLOC voffset_part1_out
.VGPR_ALLOC voffset_part2_in
.VGPR_ALLOC voffset_part2_out
.VGPR_ALLOC voffset_ldsw
accums_cnt = wei_w * wei_h * k_per_gpr * c_mult * k_mult
.VGPR_ALLOC accums, accums_cnt
single_lane_vgpr_offset = read_size

inbuf_prefetch_vgpr_offset = single_lane_vgpr_offset * c_mult
inbuf_bit_convert_vgpr_offset = inbuf_prefetch_vgpr_offset * (data_prefetch + 1)
lines_in_cnt = inbuf_bit_convert_vgpr_offset + (bit_convert_mult * inbuf_prefetch_vgpr_offset)
.VGPR_ALLOC lines_in, lines_in_cnt

outbuf_prefetch_vgpr_offset = single_lane_vgpr_offset * k_mult
outbuf_bit_convert_vgpr_offset = outbuf_prefetch_vgpr_offset * (data_prefetch + 1)
lines_out_cnt = outbuf_bit_convert_vgpr_offset + (bit_convert_mult * outbuf_prefetch_vgpr_offset)
.VGPR_ALLOC lines_out, lines_out_cnt

.VGPR_ALLOC permute_addr
.VGPR_ALLOC n_id
.if (madmix_instructions_available == 0 && dot_instructions_available == 0 && fmamix_instructions_available == 0)
    .VGPR_ALLOC vtmp_cvt_fir
    .VGPR_ALLOC vtmp_cvt_sec
.endif


static_assert (((n_part_cnt - 1) * wave_size * 4 * accums_cnt) <= 65536) //LDS size
.LDS_ALLOC_FROM 0
.LDS_ALLOC accums_lds, (n_part_cnt - 1) * wave_size * 4 * accums_cnt // lds_read_size

.GPR_ALLOC_END

max_waves_per_CU = (256 / .AUTO_VGPR_COUNT) * 4
static_assert( max_waves_per_CU >= group_size )
//.text 0
//.p2align 8
gcnAsmConv1x1WrW:

    .amd_kernel_code_t
     enable_sgpr_kernarg_segment_ptr = 1
     enable_sgpr_workgroup_id_x = 1
     enable_sgpr_workgroup_id_y = 1
     enable_sgpr_workgroup_id_z = 1
     is_ptr64 = 1
     granulated_workitem_vgpr_count = .AUTO_VGPR_GRANULATED_COUNT
     granulated_wavefront_sgpr_count = .AUTO_SGPR_GRANULATED_COUNT
     enable_vgpr_workitem_id = 1
     user_sgpr_count = 2
     kernarg_segment_byte_size = 64
     wavefront_sgpr_count = .AUTO_SGPR_COUNT
     workitem_vgpr_count = .AUTO_VGPR_COUNT
     float_mode = 192
     workgroup_group_segment_byte_size = .AUTO_LDS_BYTE_SIZE
    .end_amd_kernel_code_t

    .macro mult_acc_fp16 v_acc, v_base_out, v_base_in, it, cnt
    .if( ( (\it * elements_in_dword) + elements_in_dword) <= \cnt)
        .if(dot_instructions_available)
            v_dot2_f32_f16 v[\v_acc], v[\v_base_out], v[\v_base_in], v[\v_acc]
        .elseif (madmix_instructions_available)
            v_mad_mix_f32 v[\v_acc], v[\v_base_out], v[\v_base_in], v[\v_acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
            v_mad_mix_f32 v[\v_acc], v[\v_base_out], v[\v_base_in], v[\v_acc] op_sel:[1,1,0] op_sel_hi:[1,1,0]
        .elseif fmamix_instructions_available
            v_fma_mix_f32 v[\v_acc], v[\v_base_out], v[\v_base_in], v[\v_acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
            v_fma_mix_f32 v[\v_acc], v[\v_base_out], v[\v_base_in], v[\v_acc] op_sel:[1,1,0] op_sel_hi:[1,1,0]
        .else
            v_cvt_f32_f16 v[vtmp_cvt_fir], v[\v_base_in]
            v_cvt_f32_f16 v[vtmp_cvt_sec], v[\v_base_out]
            v_mac_f32     v[\v_acc], v[vtmp_cvt_fir], v[vtmp_cvt_sec]

            v_lshrrev_b32 v[vtmp_cvt_fir], 16, v[\v_base_in]
            v_lshrrev_b32 v[vtmp_cvt_sec], 16, v[\v_base_out]

            v_cvt_f32_f16 v[vtmp_cvt_fir], v[vtmp_cvt_fir]
            v_cvt_f32_f16 v[vtmp_cvt_sec], v[vtmp_cvt_sec]
            v_mac_f32     v[\v_acc], v[vtmp_cvt_fir], v[vtmp_cvt_sec]
        .endif
    .else   //if partial read
        .if(madmix_instructions_available)
            v_mad_mix_f32 v[\v_acc], v[\v_base_out], v[\v_base_in], v[\v_acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
        .elseif fmamix_instructions_available
            v_fma_mix_f32 v[\v_acc], v[\v_base_out], v[\v_base_in], v[\v_acc] op_sel:[0,0,0] op_sel_hi:[1,1,0]
        .else
            v_cvt_f32_f16 v[vtmp_cvt_fir], v[\v_base_in]
            v_cvt_f32_f16 v[vtmp_cvt_sec], v[\v_base_out]
            v_mac_f32     v[\v_acc], v[vtmp_cvt_fir], v[vtmp_cvt_sec]
        .endif
    .endif
.endm

    .macro bfp16_fp32_convert bfp16_vgpr_ptr, second_fp32_res_ptr, cnt
    convert_i = 0
    .rept \cnt
        //v_lshlrev_b32 v[\second_fp32_res_ptr + convert_i], 16, v[\bfp16_vgpr_ptr + convert_i]
        //v_and_b32 v[\bfp16_vgpr_ptr + convert_i], 0 + 0xFFFF0000, v[\bfp16_vgpr_ptr + convert_i]
        v_and_b32 v[\second_fp32_res_ptr + convert_i], 0 + 0xFFFF0000, v[\bfp16_vgpr_ptr + convert_i]
        v_lshlrev_b32 v[\bfp16_vgpr_ptr + convert_i], 16, v[\bfp16_vgpr_ptr + convert_i]
        
        convert_i = convert_i + 1
    .endr
.endm

.macro m_conv_accums elements_cnt, ld_part_id
    rotates_inflight = 0
    k_ds = 0
    .if(\elements_cnt == 0)
        .exitm
    .endif

    .if(buf_type == TYPE_BFP16 && bfp16_native_support == 0)
        conv_elements_cnt = \elements_cnt
        fi_element_ptr = lines_in + (\ld_part_id * inbuf_prefetch_vgpr_offset)
        bfp16_fp32_convert fi_element_ptr, lines_in + inbuf_bit_convert_vgpr_offset, inbuf_prefetch_vgpr_offset
        fi_element_ptr = lines_out + (\ld_part_id * outbuf_prefetch_vgpr_offset)
        bfp16_fp32_convert fi_element_ptr, lines_out + outbuf_bit_convert_vgpr_offset, outbuf_prefetch_vgpr_offset
    .else
        conv_elements_cnt = (\elements_cnt + elements_in_dword - 1) / elements_in_dword
    .endif

    .rept k_ds_rotates
        i = 0
        .rept conv_elements_cnt
            kx = 0
            .rept k_mult
                base_out = lines_out + kx * read_size + (\ld_part_id * outbuf_prefetch_vgpr_offset)
                .if (buf_type == TYPE_BFP16 && bfp16_native_support == 0)
                    base_out = base_out - (i % 2) * (\ld_part_id * outbuf_prefetch_vgpr_offset)
                    base_out = base_out + (i % 2) * outbuf_bit_convert_vgpr_offset + (i / 2)
                .else
                    base_out = base_out + i
                .endif
                .if k_ds > 0
                    rotates_inflight = rotates_inflight - 1
                    s_wait , rotates_inflight
                .endif

                k_dpp = 0
                .rept k_dpp_rotates
                cx = 0

                .rept c_mult
                    base_in = lines_in + cx * read_size + (\ld_part_id * inbuf_prefetch_vgpr_offset)
                    acc = accums + k_per_gpr * (cx * k_mult + kx) + k_ds * k_dpp_rotates
                    
                    .if(buf_type == TYPE_BFP16 && bfp16_native_support == 0)
                        base_in = base_in - (i % 2) * (\ld_part_id * inbuf_prefetch_vgpr_offset)
                        base_in = base_in + (i % 2) * inbuf_bit_convert_vgpr_offset + (i / 2)
                    .else
                        base_in = base_in + i
                    .endif

                    .if(elements_in_dword == 2 && ( (buf_type == TYPE_FP16) || (buf_type == TYPE_BFP16 && bfp16_native_support == 1) ))
                        .if(buf_type == TYPE_FP16)
                            mult_acc_fp16 (acc + k_dpp), (base_out), (base_in), i, \elements_cnt
                        .elseif (buf_type == TYPE_BFP16)
                            mult_acc_bfp16 (acc + k_dpp), (base_out), (base_in), i, \elements_cnt
                        .endif
                    .else   //if fp32 or converted bfp16
                        .if(k_dpp == 0)
                            v_mac_f32 v[acc], v[base_out], v[base_in]
                        .else
                            v_mac_f32 v[acc + k_dpp], v[base_out], v[base_in] row_ror:16*k_dpp/k_dpp_rotates
                        .endif
                    .endif
                    cx = cx + 1
                .endr

                    k_dpp = k_dpp + 1
                    .if(elements_in_dword == 2 && k_dpp_rotates > 1 && madmix_fmamix_with_dpp_available == 0 && buf_type != TYPE_BFP16)
                        v_mov_b32 v[base_out], v[base_out] row_ror:16/k_dpp_rotates
                        s_nop 1
                    .endif
                .endr

                .if (k_ds + 1) < k_ds_rotates
                    static_assert (c_quads == 2 || c_quads == 4)
                    .if c_quads == 2
                        ds_swizzle_b32 v[base_out], v[base_out] offset:0xc200
                    .elseif c_quads == 4
                        ds_bpermute_b32 v[base_out], v[permute_addr], v[base_out]
                    .endif
                    rotates_inflight = rotates_inflight + 1
                .endif

                kx = kx + 1
            .endr
            i = i + 1
        .endr
        k_ds = k_ds + 1
    .endr

.endm

.macro m_acc_reduction first_round, rounds
    i = 0
    .rept \rounds
        round = i + \first_round
        acc = accums
        .rept accums_cnt
            .if i >= 1 && accums_cnt <= 2
                s_nop 2 - accums_cnt
            .endif
            .if round == 0
                v_add_f32 v[acc], v[acc], v[acc] quad_perm:[1,0,3,2]
            .elseif round == 1
                v_add_f32 v[acc], v[acc], v[acc] quad_perm:[2,3,0,1]
            .elseif round == 2
                v_add_f32 v[acc], v[acc], v[acc] row_ror:12
            .elseif round == 3
                v_add_f32 v[acc], v[acc], v[acc] row_ror:8
            .elseif round == 4
                static_assert (0) //v_add_f32 v[acc], v[acc], v[acc] row_bcast:15
            .elseif round == 5
                static_assert (0) //v_add_f32 v[acc], v[acc], v[acc] row_bcast:31
            .else
                static_assert (0)
            .endif
            acc = acc + 1
        .endr
        i = i + 1
    .endr
.endm

    s_load_dwordx2 s[desc_in:desc_in+1], s[kernarg:kernarg+1], 0x0 + in_ptr_off
    s_load_dwordx2 s[desc_wei:desc_wei+1], s[kernarg:kernarg+1], 0x0 + wei_ptr_off
    s_load_dwordx2 s[desc_out:desc_out+1], s[kernarg:kernarg+1], 0x0 + out_ptr_off
    s_mov_b32 m0, -1

    v_readfirstlane_b32 s[wave_id], v[tid]
    s_lshr_b32 s[wave_id], s[wave_id], 0+wave_size_log2
    v_and_b32 v[tid], 0x3f, v[tid]

    // calculate input/output offsets
    // example for c_per_gpr=4, k_per_gpr=2, n_per_gpr=1
    // lanes  0-15: c0, k0, n0
    // lanes 16-31: c1, k0, n0
    // lanes 32-47: c2, k1, n0
    // lanes 48-63: c3, k1, n0
    vtmp = accums
    c_id = lines_in
    k_id = lines_out
    v_lshrrev_b32 v[n_id], 0 + wave_size_log2 - n_per_gpr_log2, v[tid]
    v_bfe_u32 v[c_id], v[tid], 0 + chunk_size_log2, 0 + c_per_gpr_log2
    v_bfe_u32 v[k_id], v[tid], 0 + chunk_size_log2 + c_per_gpr_log2 - k_per_gpr_log2, 0 + k_per_gpr_log2

    s_mov_b32 s[stmp], 0 + input_c_stride * sequential_c_channels
    v_mul_lo_u32 v[voffset_in], s[s